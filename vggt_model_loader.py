"""
VGGT Model Loader
ç‹¬ç«‹çš„VGGTæ¨¡å‹åŠ è½½å’Œç®¡ç†æ¨¡å—
"""

import os
import logging
from typing import Dict, Optional
import torch

# ComfyUIç›¸å…³å¯¼å…¥
try:
    import folder_paths
    import comfy.model_management
    COMFYUI_AVAILABLE = True
except ImportError:
    folder_paths = None
    comfy = None
    COMFYUI_AVAILABLE = False

# VGGTç›¸å…³å¯¼å…¥
try:
    from vggt.models.vggt import VGGT
    VGGT_AVAILABLE = True
except Exception as e:
    VGGT = None
    VGGT_AVAILABLE = False
    _VGGT_IMPORT_ERROR = e

# é…ç½®æ—¥å¿—
logger = logging.getLogger('vvl_vggt_loader')

# VGGTæ¨¡å‹é…ç½®
VGGT_MODEL_DIR_NAME = "vggt"
VGGT_MODEL_CONFIG = {
    "VGGT-1B": {
        "model_name": "facebook/VGGT-1B", 
        "description": "VGGT 1B parameter model for camera pose estimation",
        "file_name": "vggt_1b.pt",
        "size_mb": 4700  # çº¦4.7GB
    },
    # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹é…ç½®
    # "VGGT-Large": {
    #     "model_name": "facebook/VGGT-Large",
    #     "description": "VGGT Large model with better accuracy",
    #     "file_name": "vggt_large.pt"
    # }
}

# å…¨å±€æ¨¡å‹ç¼“å­˜
_VGGT_MODEL_CACHE = {}

def get_vggt_model_dir() -> str:
    """è·å–VGGTæ¨¡å‹ç›®å½•è·¯å¾„"""
    if COMFYUI_AVAILABLE and folder_paths:
        # ä½¿ç”¨ComfyUIçš„modelsç›®å½•
        model_dir = os.path.join(folder_paths.models_dir, VGGT_MODEL_DIR_NAME)
    else:
        # å¤‡ç”¨è·¯å¾„
        model_dir = os.path.join(os.path.expanduser("~"), ".cache", "comfyui", "models", VGGT_MODEL_DIR_NAME)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(model_dir, exist_ok=True)
    return model_dir

def get_vggt_model_path(model_config: dict) -> Optional[str]:
    """è·å–VGGTæ¨¡å‹çš„æœ¬åœ°è·¯å¾„"""
    model_dir = get_vggt_model_dir()
    model_path = os.path.join(model_dir, model_config["file_name"])
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(model_path):
        logger.info(f"Found local VGGT model: {model_path}")
        return model_path
    
    logger.warning(f"Local VGGT model not found: {model_path}")
    logger.info(f"Expected model directory: {model_dir}")
    return None

def list_available_models() -> Dict[str, bool]:
    """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹åŠå…¶æœ¬åœ°å¯ç”¨æ€§"""
    available_models = {}
    for model_name, config in VGGT_MODEL_CONFIG.items():
        local_path = get_vggt_model_path(config)
        available_models[model_name] = local_path is not None
    return available_models

def load_vggt_model(model_name: str, device: torch.device) -> Optional[torch.nn.Module]:
    """åŠ è½½VGGTæ¨¡å‹"""
    if not VGGT_AVAILABLE:
        logger.error(f"VGGT not available: {_VGGT_IMPORT_ERROR}")
        return None
    
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{model_name}_{device}"
        if cache_key in _VGGT_MODEL_CACHE:
            logger.info(f"Using cached VGGT model: {model_name}")
            return _VGGT_MODEL_CACHE[cache_key]
        
        if model_name not in VGGT_MODEL_CONFIG:
            raise ValueError(f"Unknown VGGT model: {model_name}. Available: {list(VGGT_MODEL_CONFIG.keys())}")
        
        config = VGGT_MODEL_CONFIG[model_name]
        local_path = get_vggt_model_path(config)
        
        # åŠ è½½æ¨¡å‹
        if local_path and os.path.exists(local_path):
            logger.info(f"Loading VGGT model from local path: {local_path}")
            # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„æœ¬åœ°åŠ è½½APIè°ƒæ•´
            # ç›®å‰å…ˆä½¿ç”¨HuggingFaceçš„æ–¹å¼ï¼Œåç»­å¯ä»¥ä¼˜åŒ–ä¸ºç›´æ¥åŠ è½½æœ¬åœ°æƒé‡
            model = VGGT.from_pretrained(config["model_name"])
        else:
            # ä»HuggingFaceåŠ è½½ï¼ˆä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
            logger.info(f"Loading VGGT model from HuggingFace: {config['model_name']}")
            logger.warning(f"Model will be downloaded (~{config.get('size_mb', 'Unknown')}MB)")
            model = VGGT.from_pretrained(config["model_name"])
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model = model.to(device)
        model.eval()
        
        # ç¼“å­˜æ¨¡å‹
        _VGGT_MODEL_CACHE[cache_key] = model
        
        logger.info(f"VGGT model {model_name} loaded successfully on {device}")
        return model
        
    except Exception as e:
        logger.error(f"Failed to load VGGT model {model_name}: {e}")
        return None

def clear_model_cache():
    """æ¸…é™¤æ¨¡å‹ç¼“å­˜"""
    global _VGGT_MODEL_CACHE
    for key in list(_VGGT_MODEL_CACHE.keys()):
        del _VGGT_MODEL_CACHE[key]
    logger.info("VGGT model cache cleared")

def get_model_info() -> Dict:
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return {
        "available": VGGT_AVAILABLE,
        "error": _VGGT_IMPORT_ERROR if not VGGT_AVAILABLE else None,
        "model_dir": get_vggt_model_dir(),
        "cached_models": list(_VGGT_MODEL_CACHE.keys()),
        "config": VGGT_MODEL_CONFIG,
        "local_models": list_available_models()
    }

class VVLVGGTLoader:
    """VGGTæ¨¡å‹åŠ è½½å™¨èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        vggt_models = list(VGGT_MODEL_CONFIG.keys())
        device_list = ["auto", "cuda", "cpu"]
        
        return {
            "required": {
                "device": (device_list, {
                    "default": "auto",
                    "tooltip": "é€‰æ‹©è¿è¡Œè®¾å¤‡ã€‚autoä¼šè‡ªåŠ¨é€‰æ‹©CUDAï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–CPU"
                }),
                "vggt_model": (vggt_models, {
                    "default": vggt_models[0] if vggt_models else "VGGT-1B",
                    "tooltip": "é€‰æ‹©VGGTæ¨¡å‹ç‰ˆæœ¬ã€‚VGGT-1Bæ˜¯æ ‡å‡†çš„10äº¿å‚æ•°æ¨¡å‹ï¼Œçº¦4.7GB"
                }),
            }
        }

    RETURN_TYPES = ("VVL_VGGT_MODEL",)
    RETURN_NAMES = ("vggt_model",)
    FUNCTION = "load_vggt_model"
    CATEGORY = "ğŸ’ƒVVL/VGGT"

    def load_vggt_model(self, device: str, vggt_model: str):
        """åŠ è½½VGGTæ¨¡å‹å¹¶è¿”å›æ¨¡å‹å®ä¾‹"""
        
        # ç¡®å®šè®¾å¤‡
        if device == "auto":
            torch_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            torch_device = torch.device(device)
        
        logger.info(f"VVLVGGTLoader: Loading VGGT model: {vggt_model} on device: {torch_device}")
        
        # æ£€æŸ¥VGGTæ˜¯å¦å¯ç”¨
        if not VGGT_AVAILABLE:
            error_msg = f"VGGT not available: {_VGGT_IMPORT_ERROR}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        model_info = get_model_info()
        logger.info(f"Model directory: {model_info['model_dir']}")
        logger.info(f"Local models: {model_info['local_models']}")
        
        # åŠ è½½æ¨¡å‹
        model_instance = load_vggt_model(vggt_model, torch_device)
        
        if model_instance is None:
            raise RuntimeError(f"Failed to load VGGT model: {vggt_model}")
        
        # åˆ›å»ºåŒ…å«æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        model_data = {
            'model': model_instance,
            'device': torch_device,
            'model_name': vggt_model,
            'config': VGGT_MODEL_CONFIG[vggt_model],
            'loader_info': model_info
        }
        
        logger.info("VVLVGGTLoader: VGGT model loaded successfully")
        return (model_data,) 