import os
import json
import tempfile
from typing import List, Any, Dict
import logging

import cv2
import numpy as np
import torch
from PIL import Image

# å°è¯•å¯¼å…¥ ComfyUI çš„ç±»å‹æ ‡è®°
try:
    from comfy.comfy_types import IO
except ImportError:
    class IO:
        VIDEO = "VIDEO"
        IMAGE = "IMAGE"

# å¯¼å…¥ VGGT ç›¸å…³å‡½æ•°
try:
    from vggt.utils.load_fn import load_and_preprocess_images
    from vggt.utils.pose_enc import pose_encoding_to_extri_intri
    VGGT_UTILS_AVAILABLE = True
except Exception as e:
    load_and_preprocess_images = None
    pose_encoding_to_extri_intri = None
    VGGT_UTILS_AVAILABLE = False
    _VGGT_UTILS_IMPORT_ERROR = e

# å¯¼å…¥æ¨¡å‹åŠ è½½å™¨
try:
    from .vggt_model_loader import VVLVGGTLoader
    MODEL_LOADER_AVAILABLE = True
except ImportError:
    VVLVGGTLoader = None
    MODEL_LOADER_AVAILABLE = False

# é…ç½®æ—¥å¿—
logger = logging.getLogger('vvl_vggt_nodes')

# -----------------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# -----------------------------------------------------------------------------

def _extract_video_frames(video_path: str, interval: int, max_frames: int) -> List[np.ndarray]:
    """æŒ‰ç…§ç»™å®šé—´éš”æå–è§†é¢‘å¸§ (BGR)."""
    cap = cv2.VideoCapture(video_path)
    frames = []
    idx = 0
    while len(frames) < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        if idx % interval == 0:
            frames.append(frame.copy())
        idx += 1
    cap.release()
    return frames

def _matrices_to_json(intrinsic, extrinsic) -> (str, str):
    """å°†ç›¸æœºçŸ©é˜µè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ã€‚"""
    num_views = extrinsic.shape[0]
    intrinsics_list = []
    poses_list = []
    for i in range(num_views):
        K = intrinsic[i].tolist()
        Rt = extrinsic[i].tolist()
        # ç›¸æœºä½ç½® world åæ ‡ ( -R^T * t )
        R = np.array(Rt)[:3, :3]
        t = np.array(Rt)[:3, 3]
        position = (-R.T @ t).tolist()
        intrinsics_list.append({
            "view_id": i,
            "intrinsic_matrix": K
        })
        poses_list.append({
            "view_id": i,
            "extrinsic_matrix": Rt,
            "position": position
        })
    return (
        json.dumps({"cameras": intrinsics_list}, ensure_ascii=False, indent=2),
        json.dumps({"poses": poses_list}, ensure_ascii=False, indent=2),
    )

def _create_traj_preview(extrinsic: torch.Tensor) -> torch.Tensor:
    """æ ¹æ®ç›¸æœºå¤–å‚åˆ›å»º3Dè½¨è¿¹å¯è§†åŒ–ï¼ˆä½¿ç”¨matplotlib 3Dç»˜å›¾ï¼‰ã€‚"""
    ext = extrinsic.cpu().numpy()  # (N,3,4)
    positions = []
    orientations = []
    
    for mat in ext:
        R = mat[:3, :3]
        t = mat[:3, 3]
        pos = -R.T @ t  # ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®
        positions.append(pos)
        # æå–ç›¸æœºæœå‘ï¼ˆZè½´æ–¹å‘ï¼‰
        forward = -R[:, 2]  # ç›¸æœºæœå‘ï¼ˆZè½´è´Ÿæ–¹å‘ï¼‰
        orientations.append(forward)
    
    positions = np.array(positions)
    orientations = np.array(orientations)
    
    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
    if len(positions) == 0:
        print("VGGT: æ²¡æœ‰æœ‰æ•ˆçš„ç›¸æœºä½å§¿æ•°æ®")
        return _create_insufficient_data_image()
    
    print(f"VGGT: å¤„ç† {len(positions)} ä¸ªç›¸æœºä½å§¿")
    
    # å³ä½¿åªæœ‰ä¸€ä¸ªä½å§¿ä¹Ÿå¯ä»¥æ˜¾ç¤º
    if len(positions) == 1:
        print("VGGT: å•ä¸ªç›¸æœºä½å§¿ï¼Œå°†åˆ›å»ºç®€åŒ–å¯è§†åŒ–")

    try:
        # ä½¿ç”¨matplotlibåˆ›å»º3Dç«‹ä½“å¯è§†åŒ–
        import matplotlib.pyplot as plt
        from mpl_toolkits.mplot3d import Axes3D
        
        fig = plt.figure(figsize=(12, 9))
        ax = fig.add_subplot(111, projection='3d')
        
        # ç»˜åˆ¶è½¨è¿¹çº¿
        if len(positions) > 1:
            ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 
                   'b-', linewidth=3, alpha=0.8, label='Camera Path')
        
        # ç”¨é¢œè‰²æ¸å˜è¡¨ç¤ºæ—¶é—´è¿›ç¨‹
        colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))
        scatter = ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], 
                           c=colors, s=60, alpha=0.9, edgecolors='black', linewidth=0.5)
        
        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        if len(positions) > 0:
            ax.scatter([positions[0, 0]], [positions[0, 1]], [positions[0, 2]], 
                      c='green', s=150, marker='^', label='Start', edgecolors='darkgreen', linewidth=2)
        if len(positions) > 1:
            ax.scatter([positions[-1, 0]], [positions[-1, 1]], [positions[-1, 2]], 
                      c='red', s=150, marker='o', label='End', edgecolors='darkred', linewidth=2)
        
        # æ·»åŠ ç›¸æœºæ–¹å‘æŒ‡ç¤ºå™¨ï¼ˆæ¯å‡ ä¸ªä½å§¿æ˜¾ç¤ºä¸€ä¸ªï¼‰
        if len(positions) > 0 and len(orientations) > 0:
            # å®‰å…¨åœ°è®¡ç®—æ˜¾ç¤ºæ­¥é•¿ï¼Œæœ€å¤šæ˜¾ç¤º10ä¸ªç®­å¤´
            step = max(1, len(positions) // 10)
            
            # è®¡ç®—åˆé€‚çš„ç®­å¤´é•¿åº¦
            position_range = positions.max(axis=0) - positions.min(axis=0)
            scene_scale = np.linalg.norm(position_range)
            
            # æ›´æ˜æ˜¾çš„ç®­å¤´é•¿åº¦è®¡ç®—ï¼Œç¡®ä¿ç®­å¤´æ¸…æ™°å¯è§
            if scene_scale < 1e-6:
                direction_length = 0.5  # æå°åœºæ™¯ä½¿ç”¨æ›´å¤§çš„é»˜è®¤é•¿åº¦
            else:
                # ä½¿ç”¨æ›´æ˜æ˜¾çš„æ¯”ä¾‹ï¼š8%~20%
                direction_length = scene_scale * 0.15  # åŸºå‡† 15%
                min_len = scene_scale * 0.08  # æœ€å°8%
                max_len = scene_scale * 0.20   # æœ€å¤§20%
                direction_length = max(min_len, min(direction_length, max_len))
                
                # è¿›ä¸€æ­¥é™åˆ¶æœ€å¤§é•¿åº¦ï¼Œé¿å…ç®­å¤´è¿‡é•¿
                absolute_max_length = scene_scale * 0.4  # ç»å¯¹ä¸è¶…è¿‡åœºæ™¯å°ºåº¦çš„40%
                direction_length = min(direction_length, absolute_max_length)
            
            # ç»˜åˆ¶ç®­å¤´ï¼Œç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
            arrow_count = 0
            for i in range(0, len(positions), step):
                if i < len(orientations) and arrow_count < 12:  # å‡å°‘åˆ°æœ€å¤š12ä¸ªç®­å¤´
                    pos = positions[i]
                    direction = orientations[i]
                    
                    # å½’ä¸€åŒ–æ–¹å‘å‘é‡ï¼Œé¿å…å¼‚å¸¸é•¿åº¦
                    direction_norm = np.linalg.norm(direction)
                    if direction_norm > 1e-6:
                        direction = direction / direction_norm
                        direction_scaled = direction * direction_length
                        
                        # æ£€æŸ¥ç®­å¤´ç»ˆç‚¹æ˜¯å¦ä¼šè¶…å‡ºåœºæ™¯è¾¹ç•Œ
                        arrow_end = pos + direction_scaled
                        scene_min = positions.min(axis=0)
                        scene_max = positions.max(axis=0)
                        
                        # å¦‚æœç®­å¤´ä¼šè¶…å‡ºè¾¹ç•Œï¼Œè¿›ä¸€æ­¥ç¼©çŸ­
                        for axis in range(3):
                            if arrow_end[axis] < scene_min[axis] or arrow_end[axis] > scene_max[axis]:
                                direction_scaled *= 0.7  # ç¼©çŸ­30%
                                break
                        
                        ax.quiver(pos[0], pos[1], pos[2], 
                                 direction_scaled[0], direction_scaled[1], direction_scaled[2], 
                                 color='orange', alpha=0.8, arrow_length_ratio=0.2, linewidth=1.5)
                        arrow_count += 1
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xlabel('X (meters)', fontsize=12)
        ax.set_ylabel('Y (meters)', fontsize=12)
        ax.set_zlabel('Z (meters)', fontsize=12)
        ax.set_title('VGGT Camera Trajectory (3D View)', fontsize=14, fontweight='bold')
        
        # æ·»åŠ å›¾ä¾‹
        ax.legend(loc='upper right', fontsize=10)
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=ax, shrink=0.6, aspect=20)
        cbar.set_label('Time Progress', fontsize=10)
        
        # è®¾ç½®ç›¸ç­‰çš„åæ ‡è½´æ¯”ä¾‹
        if len(positions) > 1:
            max_range = np.array([positions.max(axis=0) - positions.min(axis=0)]).max() / 2.0
            mid_x = (positions.max(axis=0)[0] + positions.min(axis=0)[0]) * 0.5
            mid_y = (positions.max(axis=0)[1] + positions.min(axis=0)[1]) * 0.5
            mid_z = (positions.max(axis=0)[2] + positions.min(axis=0)[2]) * 0.5
        else:
            # å•ä¸ªä½å§¿çš„æƒ…å†µï¼Œè®¾ç½®ä¸€ä¸ªåˆç†çš„æ˜¾ç¤ºèŒƒå›´
            max_range = 2.0  # é»˜è®¤2ç±³çš„æ˜¾ç¤ºèŒƒå›´
            mid_x, mid_y, mid_z = positions[0]
        
        # ç¡®ä¿èŒƒå›´ä¸ä¸ºé›¶
        if max_range < 0.1:
            max_range = 1.0
        
        ax.set_xlim(mid_x - max_range, mid_x + max_range)
        ax.set_ylim(mid_y - max_range, mid_y + max_range)
        ax.set_zlim(mid_z - max_range, mid_z + max_range)
        
        # è®¾ç½®ç½‘æ ¼
        ax.grid(True, alpha=0.3)
        
        # è°ƒæ•´è§†è§’
        ax.view_init(elev=20, azim=45)
        
        # ä¿å­˜ä¸ºå›¾åƒ
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            plt.savefig(tmp.name, dpi=120, bbox_inches='tight', facecolor='white')
            plt.close()
            
            # è¯»å–å›¾åƒ
            img = cv2.imread(tmp.name)
            os.unlink(tmp.name)
            
            if img is not None:
                # BGRè½¬RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºtorch tensor
                img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0)
                print(f"VGGT: æˆåŠŸåˆ›å»º3Dè½¨è¿¹å¯è§†åŒ–ï¼Œå›¾åƒå°ºå¯¸: {img.shape}")
                return img_tensor.unsqueeze(0)
            else:
                print("VGGT: è¯»å–ç”Ÿæˆçš„å¯è§†åŒ–å›¾åƒå¤±è´¥")
                return _create_insufficient_data_image()
    
    except Exception as e:
        print(f"VGGT: åˆ›å»º3Då¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # å¦‚æœ3Då¯è§†åŒ–å¤±è´¥ï¼Œåˆ›å»ºå¤‡ç”¨çš„2Då¯è§†åŒ–
        return _create_fallback_2d_visualization_vggt(positions, orientations)

def _create_fallback_2d_visualization_vggt(positions: np.ndarray, orientations: np.ndarray) -> torch.Tensor:
    """åˆ›å»ºVGGTå¤‡ç”¨2Då¯è§†åŒ–ï¼ˆå½“3Då¯è§†åŒ–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
    try:
        import matplotlib.pyplot as plt
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('VGGT Camera Trajectory (2D Views)', fontsize=16, fontweight='bold')
        
        # XYè§†å›¾ï¼ˆä¿¯è§†å›¾ï¼‰
        if len(positions) > 1:
            ax1.plot(positions[:, 0], positions[:, 1], 'b-', linewidth=2, alpha=0.7)
        colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))
        ax1.scatter(positions[:, 0], positions[:, 1], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax1.scatter(positions[0, 0], positions[0, 1], c='green', s=100, marker='^', label='Start')
        if len(positions) > 1:
            ax1.scatter(positions[-1, 0], positions[-1, 1], c='red', s=100, marker='o', label='End')
        ax1.set_xlabel('X (meters)')
        ax1.set_ylabel('Y (meters)')
        ax1.set_title('Top View (XY)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # XZè§†å›¾ï¼ˆä¾§è§†å›¾ï¼‰
        if len(positions) > 1:
            ax2.plot(positions[:, 0], positions[:, 2], 'g-', linewidth=2, alpha=0.7)
        ax2.scatter(positions[:, 0], positions[:, 2], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax2.scatter(positions[0, 0], positions[0, 2], c='green', s=100, marker='^')
        if len(positions) > 1:
            ax2.scatter(positions[-1, 0], positions[-1, 2], c='red', s=100, marker='o')
        ax2.set_xlabel('X (meters)')
        ax2.set_ylabel('Z (meters)')
        ax2.set_title('Side View (XZ)')
        ax2.grid(True, alpha=0.3)
        
        # YZè§†å›¾ï¼ˆæ­£è§†å›¾ï¼‰
        if len(positions) > 1:
            ax3.plot(positions[:, 1], positions[:, 2], 'r-', linewidth=2, alpha=0.7)
        ax3.scatter(positions[:, 1], positions[:, 2], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax3.scatter(positions[0, 1], positions[0, 2], c='green', s=100, marker='^')
        if len(positions) > 1:
            ax3.scatter(positions[-1, 1], positions[-1, 2], c='red', s=100, marker='o')
        ax3.set_xlabel('Y (meters)')
        ax3.set_ylabel('Z (meters)')
        ax3.set_title('Front View (YZ)')
        ax3.grid(True, alpha=0.3)
        
        # ç»Ÿè®¡ä¿¡æ¯é¢æ¿
        ax4.axis('off')
        stats_text = f"""VGGT Trajectory Statistics
        
Total Poses: {len(positions)}
Position Range:
  X: [{positions[:, 0].min():.3f}, {positions[:, 0].max():.3f}]
  Y: [{positions[:, 1].min():.3f}, {positions[:, 1].max():.3f}]
  Z: [{positions[:, 2].min():.3f}, {positions[:, 2].max():.3f}]

Path Length: {np.sum(np.linalg.norm(np.diff(positions, axis=0), axis=1)):.3f}m"""
        
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜ä¸ºå›¾åƒ
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            plt.savefig(tmp.name, dpi=120, bbox_inches='tight', facecolor='white')
            plt.close()
            
            # è¯»å–å›¾åƒ
            img = cv2.imread(tmp.name)
            os.unlink(tmp.name)
            
            if img is not None:
                # BGRè½¬RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºtorch tensor
                img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0)
                return img_tensor.unsqueeze(0)
            else:
                return _create_insufficient_data_image()
    
    except Exception as e:
        print(f"VGGT: åˆ›å»ºå¤‡ç”¨2Då¯è§†åŒ–ä¹Ÿå¤±è´¥: {e}")
        return _create_insufficient_data_image()

def _create_insufficient_data_image():
    """åˆ›å»ºæ•°æ®ä¸è¶³çš„æç¤ºå›¾åƒ"""
    canvas = np.ones((600, 800, 3), dtype=np.float32) * 0.9
    cv2.putText(canvas, "Insufficient Camera Data", (200, 280), 
                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0.3, 0.3, 0.3), 2)
    cv2.putText(canvas, "Need at least 2 frames", (250, 320), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0.5, 0.5, 0.5), 1)
    return torch.from_numpy(canvas).unsqueeze(0)

# -----------------------------------------------------------------------------
# ä¸»è¦èŠ‚ç‚¹å®ç°
# -----------------------------------------------------------------------------

class VGGTVideoCameraNode:
    """VGGT è§†é¢‘ç›¸æœºå‚æ•°ä¼°è®¡èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "vggt_model": ("VVL_VGGT_MODEL", {
                    "tooltip": "æ¥è‡ªVVLVGGTLoaderçš„VGGTæ¨¡å‹å®ä¾‹ï¼ŒåŒ…å«å·²åŠ è½½çš„æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯"
                }),
                "video": (IO.VIDEO, {
                    "tooltip": "æ¥è‡ª LoadVideo çš„è§†é¢‘å¯¹è±¡ï¼Œæˆ–ç›´æ¥è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„"
                }),
            },
            "optional": {
                "video_path": ("STRING", {
                    "default": "",
                    "tooltip": "å¤‡ç”¨è§†é¢‘è·¯å¾„ï¼Œå½“videoè¾“å…¥ä¸ºç©ºæ—¶ä½¿ç”¨"
                }),
                "frame_interval": ("INT", {
                    "default": 5, "min": 1, "max": 50, "step": 1,
                    "tooltip": "å¸§æå–é—´éš”ï¼Œæ•°å€¼è¶Šå°æå–çš„å¸§è¶Šå¯†é›†ï¼Œä½†è®¡ç®—é‡æ›´å¤§"
                }),
                "max_frames": ("INT", {
                    "default": 60, "min": 5, "max": 200, "step": 5,
                    "tooltip": "æœ€å¤§æå–å¸§æ•°ï¼Œç”¨äºæ§åˆ¶è®¡ç®—é‡å’Œå†…å­˜ä½¿ç”¨"
                }),
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING")
    RETURN_NAMES = ("intrinsics_json", "trajectory_preview", "poses_json")
    FUNCTION = "estimate"
    CATEGORY = "ğŸ’ƒVVL/VGGT"

    # ---------------------------------------------------------
    def _resolve_video_path(self, video: Any, fallback: str) -> str:
        """è§£æè§†é¢‘è·¯å¾„"""
        if video is None:
            return fallback
        # å¦‚æœ video æ˜¯å­—ç¬¦ä¸²
        if isinstance(video, str):
            return video
        # å¸¸è§å±æ€§
        attrs = ["_VideoFromFile__file", "path", "video_path", "_path", "file_path"]
        for attr in attrs:
            if hasattr(video, attr):
                val = getattr(video, attr)
                if isinstance(val, str):
                    return val
        return fallback

    # ---------------------------------------------------------
    def estimate(self, vggt_model: Dict, video=None, video_path: str = "", 
                frame_interval: int = 5, max_frames: int = 60):
        """æ‰§è¡Œç›¸æœºå‚æ•°ä¼°è®¡"""
        try:
            # æ£€æŸ¥VGGTå·¥å…·å‡½æ•°æ˜¯å¦å¯ç”¨
            if not VGGT_UTILS_AVAILABLE:
                raise RuntimeError(f"VGGT utils not available: {_VGGT_UTILS_IMPORT_ERROR}")
            
            # ä»æ¨¡å‹å­—å…¸ä¸­è·å–ä¿¡æ¯
            model_instance = vggt_model['model']
            device = vggt_model['device']
            model_name = vggt_model['model_name']
            
            logger.info(f"VGGTVideoCameraNode: Using {model_name} on {device}")
            
            # ç¡®å®šæ•°æ®ç±»å‹
            if device.type == "cuda":
                try:
                    # å°è¯•ä½¿ç”¨BFloat16ï¼Œå¦‚æœä¸æ”¯æŒåˆ™fallbackåˆ°Float16
                    dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
                except:
                    dtype = torch.float16
            else:
                dtype = torch.float32

            # è§£æè§†é¢‘è·¯å¾„
            vid_path = self._resolve_video_path(video, video_path)
            if not vid_path or not os.path.exists(vid_path):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶: {vid_path}")

            logger.info(f"VGGTVideoCameraNode: Processing video: {vid_path}")

            # æå–è§†é¢‘å¸§
            frames = _extract_video_frames(vid_path, frame_interval, max_frames)
            if not frames:
                raise RuntimeError("æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§")

            logger.info(f"VGGTVideoCameraNode: Extracted {len(frames)} frames")

            # å°†å¸§ä¿å­˜ä¸º PNG ä»¥å¤ç”¨å®˜æ–¹é¢„å¤„ç†
            with tempfile.TemporaryDirectory() as tmpdir:
                img_paths = []
                for i, frm in enumerate(frames):
                    rgb = cv2.cvtColor(frm, cv2.COLOR_BGR2RGB)
                    p = os.path.join(tmpdir, f"frame_{i:04d}.png")
                    # ä½¿ç”¨PILä¿å­˜RGBå›¾ç‰‡ï¼Œé¿å…cv2çš„BGRé—®é¢˜
                    Image.fromarray(rgb).save(p)
                    img_paths.append(p)

                # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
                imgs = load_and_preprocess_images(img_paths).to(device)
                logger.info(f"VGGTVideoCameraNode: Preprocessed images shape: {imgs.shape}")

                # æ¨¡å‹æ¨ç†
                with torch.no_grad():
                    try:
                        with torch.amp.autocast(device_type=device.type, dtype=dtype):
                            predictions = model_instance(imgs)
                    except:
                        # Fallbackæ–¹æ¡ˆ
                        try:
                            if device.type == "cuda":
                                with torch.cuda.amp.autocast(dtype=dtype):
                                    predictions = model_instance(imgs)
                            else:
                                predictions = model_instance(imgs)
                        except:
                            # æœ€åçš„fallback
                            predictions = model_instance(imgs)
                    
                    # ä»predictionsä¸­æå–pose_enc
                    pose_enc = predictions["pose_enc"]
                    logger.info(f"VGGTVideoCameraNode: pose_enc shape: {pose_enc.shape}")
                            
                # è½¬æ¢ä¸ºå†…å¤–å‚çŸ©é˜µ
                extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, imgs.shape[-2:])
                
                # å»é™¤æ‰¹æ¬¡ç»´åº¦
                if len(extrinsic.shape) == 4:  # (1,N,3,4)
                    extrinsic = extrinsic[0]   # (N,3,4)
                if len(intrinsic.shape) == 4:  # (1,N,3,3)
                    intrinsic = intrinsic[0]   # (N,3,3)
                    
                extrinsic = extrinsic.cpu()
                intrinsic = intrinsic.cpu()
                
                logger.info(f"VGGTVideoCameraNode: Final matrix shapes - "
                          f"extrinsic: {extrinsic.shape}, intrinsic: {intrinsic.shape}")

            # ç”ŸæˆJSONè¾“å‡º
            intrinsics_json, poses_json = _matrices_to_json(intrinsic.numpy(), extrinsic.numpy())

            # ç”Ÿæˆè½¨è¿¹é¢„è§ˆå›¾
            traj_tensor = _create_traj_preview(extrinsic)

            logger.info("VGGTVideoCameraNode: Camera estimation completed successfully")
            return (intrinsics_json, traj_tensor, poses_json)

        except Exception as e:
            error_msg = f"VGGTä¼°è®¡é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            
            # è¿”å›é”™è¯¯ç»“æœ
            empty_img = torch.ones((1, 400, 400, 3), dtype=torch.float32) * 0.1
            error_json = json.dumps({"success": False, "error": error_msg}, ensure_ascii=False, indent=2)
            return (error_json, empty_img, error_json)

# -----------------------------------------------------------------------------
# èŠ‚ç‚¹æ³¨å†Œ
# -----------------------------------------------------------------------------

NODE_CLASS_MAPPINGS = {
    "VGGTVideoCameraNode": VGGTVideoCameraNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VGGTVideoCameraNode": "VVL VGGT Video Camera Estimator",
}

# å¦‚æœæ¨¡å‹åŠ è½½å™¨å¯ç”¨ï¼Œæ·»åŠ åˆ°æ˜ å°„ä¸­
if MODEL_LOADER_AVAILABLE:
    NODE_CLASS_MAPPINGS["VVLVGGTLoader"] = VVLVGGTLoader
    NODE_DISPLAY_NAME_MAPPINGS["VVLVGGTLoader"] = "VVL VGGT Model Loader" 