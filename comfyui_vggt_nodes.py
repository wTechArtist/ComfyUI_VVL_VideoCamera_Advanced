import os
import json
import tempfile
from typing import List, Any, Dict
import logging
import struct

import cv2
import numpy as np
import torch
from PIL import Image

# å°è¯•å¯¼å…¥ ComfyUI çš„ç±»å‹æ ‡è®°
try:
    from comfy.comfy_types import IO
except ImportError:
    class IO:
        VIDEO = "VIDEO"
        IMAGE = "IMAGE"

# å¯¼å…¥ VGGT ç›¸å…³å‡½æ•°
try:
    from vggt.utils.load_fn import load_and_preprocess_images
    from vggt.utils.pose_enc import pose_encoding_to_extri_intri
    from vggt.utils.geometry import unproject_depth_map_to_point_map
    VGGT_UTILS_AVAILABLE = True
except Exception as e:
    load_and_preprocess_images = None
    pose_encoding_to_extri_intri = None
    unproject_depth_map_to_point_map = None
    VGGT_UTILS_AVAILABLE = False
    _VGGT_UTILS_IMPORT_ERROR = e

# å¯¼å…¥æ¨¡å‹åŠ è½½å™¨
try:
    from .vggt_model_loader import VVLVGGTLoader
    MODEL_LOADER_AVAILABLE = True
except ImportError:
    VVLVGGTLoader = None
    MODEL_LOADER_AVAILABLE = False

# å¯¼å…¥ComfyUIçš„è·¯å¾„ç®¡ç†
try:
    import folder_paths
    FOLDER_PATHS_AVAILABLE = True
except ImportError:
    folder_paths = None
    FOLDER_PATHS_AVAILABLE = False

# åŸç‰ˆVGGTçš„dependencies
try:
    import trimesh
    TRIMESH_AVAILABLE = True
except ImportError:
    TRIMESH_AVAILABLE = False
    _TRIMESH_IMPORT_ERROR = "trimesh not available"

try:
    import matplotlib
    import matplotlib.colormaps
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    _MATPLOTLIB_IMPORT_ERROR = "matplotlib not available"

try:
    from scipy.spatial.transform import Rotation
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    _SCIPY_IMPORT_ERROR = "scipy not available"

try:
    import copy
    import requests
    EXTRA_DEPS_AVAILABLE = True
except ImportError:
    EXTRA_DEPS_AVAILABLE = False

# é…ç½®æ—¥å¿—
logger = logging.getLogger('vvl_vggt_nodes')

# -----------------------------------------------------------------------------
# GLBæ–‡ä»¶ç”Ÿæˆå‡½æ•°ï¼ˆåŸºäºComfyUIå†…ç½®åŠŸèƒ½ï¼‰
# -----------------------------------------------------------------------------

def save_glb(vertices, faces, filepath, colors=None, metadata=None):
    """
    å°†é¡¶ç‚¹å’Œé¢ä¿å­˜ä¸ºGLBæ–‡ä»¶ï¼ˆåŸºäºComfyUIå†…ç½®åŠŸèƒ½ï¼Œå¢å¼ºç‰ˆæ”¯æŒé¢œè‰²ï¼‰
    
    Parameters:
    vertices: numpy.ndarray of shape (N, 3) - é¡¶ç‚¹åæ ‡
    faces: numpy.ndarray of shape (M, 3) - é¢ç´¢å¼•ï¼ˆä¸‰è§’å½¢é¢ï¼‰
    filepath: str - è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆåº”è¯¥ä»¥.glbç»“å°¾ï¼‰
    colors: numpy.ndarray of shape (N, 3) - é¡¶ç‚¹é¢œè‰²ï¼ˆå¯é€‰ï¼‰
    metadata: dict - å¯é€‰çš„å…ƒæ•°æ®
    """
    
    try:
        logger.info(f"save_glb: å¼€å§‹ä¿å­˜GLBæ–‡ä»¶åˆ° {filepath}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(filepath)
        if not os.path.exists(output_dir):
            logger.info(f"save_glb: åˆ›å»ºè¾“å‡ºç›®å½• {output_dir}")
            os.makedirs(output_dir, exist_ok=True)
        
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if isinstance(vertices, torch.Tensor):
            vertices = vertices.cpu().numpy()
        if isinstance(faces, torch.Tensor):
            faces = faces.cpu().numpy()
        if colors is not None and isinstance(colors, torch.Tensor):
            colors = colors.cpu().numpy()
        
        vertices_np = vertices.astype(np.float32)
        faces_np = faces.astype(np.uint32)
        
        logger.info(f"save_glb: é¡¶ç‚¹å½¢çŠ¶ {vertices_np.shape}, é¢å½¢çŠ¶ {faces_np.shape}")
        if colors is not None:
            colors_np = colors.astype(np.float32)
            logger.info(f"save_glb: é¢œè‰²å½¢çŠ¶ {colors_np.shape}")
    
        vertices_buffer = vertices_np.tobytes()
        indices_buffer = faces_np.tobytes()
        
        # å¤„ç†é¢œè‰²æ•°æ®
        colors_buffer = b''
        if colors is not None:
            colors_buffer = colors_np.tobytes()

        def pad_to_4_bytes(buffer):
            padding_length = (4 - (len(buffer) % 4)) % 4
            return buffer + b'\x00' * padding_length

        vertices_buffer_padded = pad_to_4_bytes(vertices_buffer)
        indices_buffer_padded = pad_to_4_bytes(indices_buffer)
        colors_buffer_padded = pad_to_4_bytes(colors_buffer) if colors is not None else b''

        buffer_data = vertices_buffer_padded + colors_buffer_padded + indices_buffer_padded

        vertices_byte_length = len(vertices_buffer)
        vertices_byte_offset = 0
        colors_byte_length = len(colors_buffer)
        colors_byte_offset = len(vertices_buffer_padded)
        indices_byte_length = len(indices_buffer)
        indices_byte_offset = len(vertices_buffer_padded) + len(colors_buffer_padded)

        # æ„å»ºbuffer views
        buffer_views = [
            {
                "buffer": 0,
                "byteOffset": vertices_byte_offset,
                "byteLength": vertices_byte_length,
                "target": 34962  # ARRAY_BUFFER
            }
        ]
        
        # æ·»åŠ é¢œè‰²buffer view
        if colors is not None:
            buffer_views.append({
                "buffer": 0,
                "byteOffset": colors_byte_offset,
                "byteLength": colors_byte_length,
                "target": 34962  # ARRAY_BUFFER
            })
        
        # æ·»åŠ ç´¢å¼•buffer view
        buffer_views.append({
            "buffer": 0,
            "byteOffset": indices_byte_offset,
            "byteLength": indices_byte_length,
            "target": 34963  # ELEMENT_ARRAY_BUFFER
        })

        # æ„å»ºaccessors
        accessors = [
            {
                "bufferView": 0,
                "byteOffset": 0,
                "componentType": 5126,  # FLOAT
                "count": len(vertices_np),
                "type": "VEC3",
                "max": vertices_np.max(axis=0).tolist(),
                "min": vertices_np.min(axis=0).tolist()
            }
        ]
        
        # æ·»åŠ é¢œè‰²accessor
        if colors is not None:
            accessors.append({
                "bufferView": 1,
                "byteOffset": 0,
                "componentType": 5126,  # FLOAT
                "count": len(colors_np),
                "type": "VEC3",
                "max": colors_np.max(axis=0).tolist(),
                "min": colors_np.min(axis=0).tolist()
            })
        
        # æ·»åŠ ç´¢å¼•accessor
        indices_accessor_index = 2 if colors is not None else 1
        accessors.append({
            "bufferView": indices_accessor_index,
            "byteOffset": 0,
            "componentType": 5125,  # UNSIGNED_INT
            "count": faces_np.size,
            "type": "SCALAR"
        })

        # æ„å»ºmesh attributes
        attributes = {"POSITION": 0}
        if colors is not None:
            attributes["COLOR_0"] = 1

        gltf = {
            "asset": {"version": "2.0", "generator": "ComfyUI-VGGT"},
            "buffers": [
                {
                    "byteLength": len(buffer_data)
                }
            ],
            "bufferViews": buffer_views,
            "accessors": accessors,
            "meshes": [
                {
                    "primitives": [
                        {
                            "attributes": attributes,
                            "indices": indices_accessor_index,
                            "mode": 4  # TRIANGLES
                        }
                    ]
                }
            ],
            "nodes": [
                {
                    "mesh": 0
                }
            ],
            "scenes": [
                {
                    "nodes": [0]
                }
            ],
            "scene": 0
        }

        if metadata is not None:
            gltf["asset"]["extras"] = metadata

        # Convert the JSON to bytes
        gltf_json = json.dumps(gltf).encode('utf8')

        def pad_json_to_4_bytes(buffer):
            padding_length = (4 - (len(buffer) % 4)) % 4
            return buffer + b' ' * padding_length

        gltf_json_padded = pad_json_to_4_bytes(gltf_json)

        # Create the GLB header
        # Magic glTF
        glb_header = struct.pack('<4sII', b'glTF', 2, 12 + 8 + len(gltf_json_padded) + 8 + len(buffer_data))

        # Create JSON chunk header (chunk type 0)
        json_chunk_header = struct.pack('<II', len(gltf_json_padded), 0x4E4F534A)  # "JSON" in little endian

        # Create BIN chunk header (chunk type 1)
        bin_chunk_header = struct.pack('<II', len(buffer_data), 0x004E4942)  # "BIN\0" in little endian

        # Write the GLB file
        logger.info(f"save_glb: å¼€å§‹å†™å…¥GLBæ–‡ä»¶")
        with open(filepath, 'wb') as f:
            f.write(glb_header)
            f.write(json_chunk_header)
            f.write(gltf_json_padded)
            f.write(bin_chunk_header)
            f.write(buffer_data)

        logger.info(f"save_glb: GLBæ–‡ä»¶å†™å…¥å®Œæˆ")
        return filepath
    
    except Exception as e:
        logger.error(f"save_glb: åˆå§‹åŒ–å¤±è´¥ {e}")
        raise e

# -----------------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# -----------------------------------------------------------------------------

def _matrices_to_json(intrinsic, extrinsic, source_type="video") -> (str, str):
    """å°†ç›¸æœºçŸ©é˜µè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ã€‚"""
    num_views = extrinsic.shape[0]
    intrinsics_list = []
    poses_list = []
    for i in range(num_views):
        K = intrinsic[i].tolist()
        Rt = extrinsic[i].tolist()
        # ç›¸æœºä½ç½® world åæ ‡ ( -R^T * t )
        R = np.array(Rt)[:3, :3]
        t = np.array(Rt)[:3, 3]
        position = (-R.T @ t).tolist()
        intrinsics_list.append({
            "view_id": i,  # view_idå¯¹åº”è¾“å…¥åºåˆ—ä¸­çš„ç´¢å¼•
            "intrinsic_matrix": K
        })
        poses_list.append({
            "view_id": i,  # view_idå¯¹åº”è¾“å…¥åºåˆ—ä¸­çš„ç´¢å¼•
            "extrinsic_matrix": Rt,
            "position": position
        })
    
    # æ ¹æ®æºç±»å‹ç”Ÿæˆä¸åŒçš„å…ƒæ•°æ®è¯´æ˜
    if source_type == "images":
        description = "view_idå¯¹åº”è¾“å…¥å›¾ç‰‡åºåˆ—ä¸­çš„ç´¢å¼•"
        note = "view_id=0å¯¹åº”ç¬¬ä¸€å¼ è¾“å…¥å›¾ç‰‡ï¼Œview_id=1å¯¹åº”ç¬¬äºŒå¼ è¾“å…¥å›¾ç‰‡ï¼Œä»¥æ­¤ç±»æ¨"
    else:  # video
        description = "view_idå¯¹åº”æå–çš„å¸§åºå·ï¼Œä¸æ˜¯åŸè§†é¢‘çš„å¸§ç´¢å¼•"
        note = "åŸç‰ˆGradioæ¨¡å¼ï¼šå›ºå®šæ¯ç§’1å¸§æå–ï¼Œæ— å¸§æ•°é™åˆ¶ï¼Œå¤„ç†æ•´ä¸ªè§†é¢‘"
    
    # ç›¸æœºå†…å‚çŸ©é˜µæ ¼å¼è¯´æ˜
    intrinsic_format_info = {
        "matrix_format": "3x3ç›¸æœºå†…å‚çŸ©é˜µï¼ŒOpenCVæ ‡å‡†æ ¼å¼",
        "matrix_structure": [
            ["fx",  "0",  "cx"],
            ["0",   "fy", "cy"],
            ["0",   "0",  "1"]
        ],
        "parameters": {
            "fx": "Xè½´ç„¦è·ï¼ˆåƒç´ å•ä½ï¼‰- å›¾åƒå®½åº¦æ–¹å‘çš„ç„¦è·",
            "fy": "Yè½´ç„¦è·ï¼ˆåƒç´ å•ä½ï¼‰- å›¾åƒé«˜åº¦æ–¹å‘çš„ç„¦è·",
            "cx": "ä¸»ç‚¹Xåæ ‡ï¼ˆåƒç´ å•ä½ï¼‰- å…‰è½´ä¸å›¾åƒå¹³é¢äº¤ç‚¹çš„Xåæ ‡",
            "cy": "ä¸»ç‚¹Yåæ ‡ï¼ˆåƒç´ å•ä½ï¼‰- å…‰è½´ä¸å›¾åƒå¹³é¢äº¤ç‚¹çš„Yåæ ‡"
        },
        "coordinate_system": "OpenCVå›¾åƒåæ ‡ç³»ï¼šåŸç‚¹åœ¨å·¦ä¸Šè§’ï¼ŒXè½´å‘å³ï¼ŒYè½´å‘ä¸‹",
        "units": "æ‰€æœ‰å‚æ•°å•ä½ä¸ºåƒç´ (pixels)"
    }
    
    # ç›¸æœºå¤–å‚çŸ©é˜µæ ¼å¼è¯´æ˜
    extrinsic_format_info = {
        "matrix_format": "3x4ç›¸æœºå¤–å‚çŸ©é˜µ [R|t]ï¼Œä¸–ç•Œåæ ‡ç³»åˆ°ç›¸æœºåæ ‡ç³»çš„å˜æ¢",
        "matrix_structure": [
            ["r11", "r12", "r13", "tx"],
            ["r21", "r22", "r23", "ty"],
            ["r31", "r32", "r33", "tz"]
        ],
        "components": {
            "R": "3x3æ—‹è½¬çŸ©é˜µ - ä¸–ç•Œåæ ‡ç³»åˆ°ç›¸æœºåæ ‡ç³»çš„æ—‹è½¬å˜æ¢",
            "t": "3x1å¹³ç§»å‘é‡ - ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®ï¼ˆç»è¿‡æ—‹è½¬å˜æ¢ï¼‰"
        },
        "coordinate_system": {
            "world_frame": "ä¸–ç•Œåæ ‡ç³»ï¼šZè½´å‘ä¸Šï¼ŒXè½´å‘å‰ï¼ŒYè½´å‘å·¦ï¼ˆå³æ‰‹åæ ‡ç³»ï¼‰",
            "camera_frame": "ç›¸æœºåæ ‡ç³»ï¼šZè½´å‘å‰ï¼ˆå…‰è½´æ–¹å‘ï¼‰ï¼ŒXè½´å‘å³ï¼ŒYè½´å‘ä¸‹"
        },
        "position_calculation": "ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„å®é™…ä½ç½® = -R^T * t",
        "units": "å¹³ç§»å‘é‡å•ä½ä¸ºç±³(meters)ï¼Œæ—‹è½¬çŸ©é˜µæ— é‡çº²"
    }
    
    # æ·»åŠ å…ƒæ•°æ®è¯´æ˜
    intrinsics_data = {
        "cameras": intrinsics_list,
        "metadata": {
            "source_type": source_type,
            "description": description,
            "note": note,
            "total_views": num_views,
            "format_specification": intrinsic_format_info
        }
    }
    
    poses_data = {
        "poses": poses_list,
        "metadata": {
            "source_type": source_type,
            "description": description,
            "note": note,
            "total_views": num_views,
            "format_specification": extrinsic_format_info
        }
    }
    
    return (
        json.dumps(intrinsics_data, ensure_ascii=False, indent=2),
        json.dumps(poses_data, ensure_ascii=False, indent=2),
    )

def _create_traj_preview(extrinsic: torch.Tensor) -> torch.Tensor:
    """æ ¹æ®ç›¸æœºå¤–å‚åˆ›å»º3Dè½¨è¿¹å¯è§†åŒ–ï¼ˆä½¿ç”¨matplotlib 3Dç»˜å›¾ï¼‰ã€‚"""
    ext = extrinsic.cpu().numpy()  # (N,3,4)
    positions = []
    orientations = []
    
    for mat in ext:
        R = mat[:3, :3]
        t = mat[:3, 3]
        pos = -R.T @ t  # ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®
        positions.append(pos)
        # æå–ç›¸æœºæœå‘ï¼ˆZè½´æ–¹å‘ï¼‰
        forward = -R[:, 2]  # ç›¸æœºæœå‘ï¼ˆZè½´è´Ÿæ–¹å‘ï¼‰
        orientations.append(forward)
    
    positions = np.array(positions)
    orientations = np.array(orientations)
    
    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
    if len(positions) == 0:
        print("VGGT: æ²¡æœ‰æœ‰æ•ˆçš„ç›¸æœºä½å§¿æ•°æ®")
        return _create_insufficient_data_image()
    
    print(f"VGGT: å¤„ç† {len(positions)} ä¸ªç›¸æœºä½å§¿")
    
    # å³ä½¿åªæœ‰ä¸€ä¸ªä½å§¿ä¹Ÿå¯ä»¥æ˜¾ç¤º
    if len(positions) == 1:
        print("VGGT: å•ä¸ªç›¸æœºä½å§¿ï¼Œå°†åˆ›å»ºç®€åŒ–å¯è§†åŒ–")

    try:
        # ä½¿ç”¨matplotlibåˆ›å»º3Dç«‹ä½“å¯è§†åŒ–
        import matplotlib.pyplot as plt
        from mpl_toolkits.mplot3d import Axes3D
        
        fig = plt.figure(figsize=(12, 9))
        ax = fig.add_subplot(111, projection='3d')
        
        # ç»˜åˆ¶è½¨è¿¹çº¿
        if len(positions) > 1:
            ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 
                   'b-', linewidth=3, alpha=0.8, label='Camera Path')
        
        # ç”¨é¢œè‰²æ¸å˜è¡¨ç¤ºæ—¶é—´è¿›ç¨‹
        colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))
        scatter = ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], 
                           c=colors, s=60, alpha=0.9, edgecolors='black', linewidth=0.5)
        
        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        if len(positions) > 0:
            ax.scatter([positions[0, 0]], [positions[0, 1]], [positions[0, 2]], 
                      c='green', s=150, marker='^', label='Start', edgecolors='darkgreen', linewidth=2)
        if len(positions) > 1:
            ax.scatter([positions[-1, 0]], [positions[-1, 1]], [positions[-1, 2]], 
                      c='red', s=150, marker='o', label='End', edgecolors='darkred', linewidth=2)
        
        # æ·»åŠ ç›¸æœºæ–¹å‘æŒ‡ç¤ºå™¨ï¼ˆæ¯å‡ ä¸ªä½å§¿æ˜¾ç¤ºä¸€ä¸ªï¼‰
        if len(positions) > 0 and len(orientations) > 0:
            # å®‰å…¨åœ°è®¡ç®—æ˜¾ç¤ºæ­¥é•¿ï¼Œæœ€å¤šæ˜¾ç¤º10ä¸ªç®­å¤´
            step = max(1, len(positions) // 10)
            
            # è®¡ç®—åˆé€‚çš„ç®­å¤´é•¿åº¦
            position_range = positions.max(axis=0) - positions.min(axis=0)
            scene_scale = np.linalg.norm(position_range)
            
            # æ›´æ˜æ˜¾çš„ç®­å¤´é•¿åº¦è®¡ç®—ï¼Œç¡®ä¿ç®­å¤´æ¸…æ™°å¯è§
            if scene_scale < 1e-6:
                direction_length = 0.5  # æå°åœºæ™¯ä½¿ç”¨æ›´å¤§çš„é»˜è®¤é•¿åº¦
            else:
                # ä½¿ç”¨æ›´æ˜æ˜¾çš„æ¯”ä¾‹ï¼š8%~20%
                direction_length = scene_scale * 0.15  # åŸºå‡† 15%
                min_len = scene_scale * 0.08  # æœ€å°8%
                max_len = scene_scale * 0.20   # æœ€å¤§20%
                direction_length = max(min_len, min(direction_length, max_len))
                
                # è¿›ä¸€æ­¥é™åˆ¶æœ€å¤§é•¿åº¦ï¼Œé¿å…ç®­å¤´è¿‡é•¿
                absolute_max_length = scene_scale * 0.4  # ç»å¯¹ä¸è¶…è¿‡åœºæ™¯å°ºåº¦çš„40%
                direction_length = min(direction_length, absolute_max_length)
            
            # ç»˜åˆ¶ç®­å¤´ï¼Œç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
            arrow_count = 0
            for i in range(0, len(positions), step):
                if i < len(orientations) and arrow_count < 12:  # å‡å°‘åˆ°æœ€å¤š12ä¸ªç®­å¤´
                    pos = positions[i]
                    direction = orientations[i]
                    
                    # å½’ä¸€åŒ–æ–¹å‘å‘é‡ï¼Œé¿å…å¼‚å¸¸é•¿åº¦
                    direction_norm = np.linalg.norm(direction)
                    if direction_norm > 1e-6:
                        direction = direction / direction_norm
                        direction_scaled = direction * direction_length
                        
                        # æ£€æŸ¥ç®­å¤´ç»ˆç‚¹æ˜¯å¦ä¼šè¶…å‡ºåœºæ™¯è¾¹ç•Œ
                        arrow_end = pos + direction_scaled
                        scene_min = positions.min(axis=0)
                        scene_max = positions.max(axis=0)
                        
                        # å¦‚æœç®­å¤´ä¼šè¶…å‡ºè¾¹ç•Œï¼Œè¿›ä¸€æ­¥ç¼©çŸ­
                        for axis in range(3):
                            if arrow_end[axis] < scene_min[axis] or arrow_end[axis] > scene_max[axis]:
                                direction_scaled *= 0.7  # ç¼©çŸ­30%
                                break
                        
                        ax.quiver(pos[0], pos[1], pos[2], 
                                 direction_scaled[0], direction_scaled[1], direction_scaled[2], 
                                 color='orange', alpha=0.8, arrow_length_ratio=0.2, linewidth=1.5)
                        arrow_count += 1
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xlabel('X (meters)', fontsize=12)
        ax.set_ylabel('Y (meters)', fontsize=12)
        ax.set_zlabel('Z (meters)', fontsize=12)
        ax.set_title('VGGT Camera Trajectory (3D View)', fontsize=14, fontweight='bold')
        
        # æ·»åŠ å›¾ä¾‹
        ax.legend(loc='upper right', fontsize=10)
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=ax, shrink=0.6, aspect=20)
        cbar.set_label('Time Progress', fontsize=10)
        
        # è®¾ç½®ç›¸ç­‰çš„åæ ‡è½´æ¯”ä¾‹
        if len(positions) > 1:
            max_range = np.array([positions.max(axis=0) - positions.min(axis=0)]).max() / 2.0
            mid_x = (positions.max(axis=0)[0] + positions.min(axis=0)[0]) * 0.5
            mid_y = (positions.max(axis=0)[1] + positions.min(axis=0)[1]) * 0.5
            mid_z = (positions.max(axis=0)[2] + positions.min(axis=0)[2]) * 0.5
        else:
            # å•ä¸ªä½å§¿çš„æƒ…å†µï¼Œè®¾ç½®ä¸€ä¸ªåˆç†çš„æ˜¾ç¤ºèŒƒå›´
            max_range = 2.0  # é»˜è®¤2ç±³çš„æ˜¾ç¤ºèŒƒå›´
            mid_x, mid_y, mid_z = positions[0]
        
        # ç¡®ä¿èŒƒå›´ä¸ä¸ºé›¶
        if max_range < 0.1:
            max_range = 1.0
        
        ax.set_xlim(mid_x - max_range, mid_x + max_range)
        ax.set_ylim(mid_y - max_range, mid_y + max_range)
        ax.set_zlim(mid_z - max_range, mid_z + max_range)
        
        # è®¾ç½®ç½‘æ ¼
        ax.grid(True, alpha=0.3)
        
        # è°ƒæ•´è§†è§’
        ax.view_init(elev=20, azim=45)
        
        # ä¿å­˜ä¸ºå›¾åƒ
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            plt.savefig(tmp.name, dpi=120, bbox_inches='tight', facecolor='white')
            plt.close()
            
            # è¯»å–å›¾åƒ
            img = cv2.imread(tmp.name)
            os.unlink(tmp.name)
            
            if img is not None:
                # BGRè½¬RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºtorch tensor
                img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0)
                print(f"VGGT: æˆåŠŸåˆ›å»º3Dè½¨è¿¹å¯è§†åŒ–ï¼Œå›¾åƒå°ºå¯¸: {img.shape}")
                return img_tensor.unsqueeze(0)
            else:
                print("VGGT: è¯»å–ç”Ÿæˆçš„å¯è§†åŒ–å›¾åƒå¤±è´¥")
                return _create_insufficient_data_image()
    
    except Exception as e:
        print(f"VGGT: åˆ›å»º3Då¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # å¦‚æœ3Då¯è§†åŒ–å¤±è´¥ï¼Œåˆ›å»ºå¤‡ç”¨çš„2Då¯è§†åŒ–
        return _create_fallback_2d_visualization_vggt(positions, orientations)

def _create_fallback_2d_visualization_vggt(positions: np.ndarray, orientations: np.ndarray) -> torch.Tensor:
    """åˆ›å»ºVGGTå¤‡ç”¨2Då¯è§†åŒ–ï¼ˆå½“3Då¯è§†åŒ–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
    try:
        import matplotlib.pyplot as plt
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('VGGT Camera Trajectory (2D Views)', fontsize=16, fontweight='bold')
        
        # XYè§†å›¾ï¼ˆä¿¯è§†å›¾ï¼‰
        if len(positions) > 1:
            ax1.plot(positions[:, 0], positions[:, 1], 'b-', linewidth=2, alpha=0.7)
        colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))
        ax1.scatter(positions[:, 0], positions[:, 1], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax1.scatter(positions[0, 0], positions[0, 1], c='green', s=100, marker='^', label='Start')
        if len(positions) > 1:
            ax1.scatter(positions[-1, 0], positions[-1, 1], c='red', s=100, marker='o', label='End')
        ax1.set_xlabel('X (meters)')
        ax1.set_ylabel('Y (meters)')
        ax1.set_title('Top View (XY)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # XZè§†å›¾ï¼ˆä¾§è§†å›¾ï¼‰
        if len(positions) > 1:
            ax2.plot(positions[:, 0], positions[:, 2], 'g-', linewidth=2, alpha=0.7)
        ax2.scatter(positions[:, 0], positions[:, 2], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax2.scatter(positions[0, 0], positions[0, 2], c='green', s=100, marker='^')
        if len(positions) > 1:
            ax2.scatter(positions[-1, 0], positions[-1, 2], c='red', s=100, marker='o')
        ax2.set_xlabel('X (meters)')
        ax2.set_ylabel('Z (meters)')
        ax2.set_title('Side View (XZ)')
        ax2.grid(True, alpha=0.3)
        
        # YZè§†å›¾ï¼ˆæ­£è§†å›¾ï¼‰
        if len(positions) > 1:
            ax3.plot(positions[:, 1], positions[:, 2], 'r-', linewidth=2, alpha=0.7)
        ax3.scatter(positions[:, 1], positions[:, 2], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax3.scatter(positions[0, 1], positions[0, 2], c='green', s=100, marker='^')
        if len(positions) > 1:
            ax3.scatter(positions[-1, 1], positions[-1, 2], c='red', s=100, marker='o')
        ax3.set_xlabel('Y (meters)')
        ax3.set_ylabel('Z (meters)')
        ax3.set_title('Front View (YZ)')
        ax3.grid(True, alpha=0.3)
        
        # ç»Ÿè®¡ä¿¡æ¯é¢æ¿
        ax4.axis('off')
        stats_text = f"""VGGT Trajectory Statistics
        
Total Poses: {len(positions)}
Position Range:
  X: [{positions[:, 0].min():.3f}, {positions[:, 0].max():.3f}]
  Y: [{positions[:, 1].min():.3f}, {positions[:, 1].max():.3f}]
  Z: [{positions[:, 2].min():.3f}, {positions[:, 2].max():.3f}]

Path Length: {np.sum(np.linalg.norm(np.diff(positions, axis=0), axis=1)):.3f}m"""
        
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜ä¸ºå›¾åƒ
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            plt.savefig(tmp.name, dpi=120, bbox_inches='tight', facecolor='white')
            plt.close()
            
            # è¯»å–å›¾åƒ
            img = cv2.imread(tmp.name)
            os.unlink(tmp.name)
            
            if img is not None:
                # BGRè½¬RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºtorch tensor
                img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0)
                return img_tensor.unsqueeze(0)
            else:
                return _create_insufficient_data_image()
    
    except Exception as e:
        print(f"VGGT: åˆ›å»ºå¤‡ç”¨2Då¯è§†åŒ–ä¹Ÿå¤±è´¥: {e}")
        return _create_insufficient_data_image()

def _create_insufficient_data_image():
    """åˆ›å»ºæ•°æ®ä¸è¶³çš„æç¤ºå›¾åƒ"""
    canvas = np.ones((600, 800, 3), dtype=np.float32) * 0.9
    cv2.putText(canvas, "Insufficient Camera Data", (200, 280), 
                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0.3, 0.3, 0.3), 2)
    cv2.putText(canvas, "Need at least 2 frames", (250, 320), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0.5, 0.5, 0.5), 1)
    return torch.from_numpy(canvas).unsqueeze(0)

def _run_vggt_model_inference(image_paths: List[str], model_instance, device) -> Dict:
    """
    è¿è¡ŒVGGTæ¨¡å‹æ¨ç†ï¼Œå‚è€ƒGradioä»£ç ä¸­çš„run_modelå‡½æ•°
    """
    try:
        # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
        images = load_and_preprocess_images(image_paths).to(device)
        logger.info(f"é¢„å¤„ç†å›¾åƒå½¢çŠ¶: {images.shape}")
        
        # ç¡®å®šæ•°æ®ç±»å‹
        dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            with torch.cuda.amp.autocast(dtype=dtype):
                predictions = model_instance(images)
        
        # è½¬æ¢pose encodingä¸ºå¤–å‚å’Œå†…å‚çŸ©é˜µ
        logger.info("è½¬æ¢pose encodingä¸ºå¤–å‚å’Œå†…å‚çŸ©é˜µ...")
        extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions["pose_enc"], images.shape[-2:])
        predictions["extrinsic"] = extrinsic
        predictions["intrinsic"] = intrinsic
        
        # è½¬æ¢tensorsä¸ºnumpy
        for key in predictions.keys():
            if isinstance(predictions[key], torch.Tensor):
                predictions[key] = predictions[key].cpu().numpy().squeeze(0)  # ç§»é™¤batchç»´åº¦
        
        # ä»æ·±åº¦å›¾ç”Ÿæˆä¸–ç•Œåæ ‡ç‚¹
        logger.info("ä»æ·±åº¦å›¾è®¡ç®—ä¸–ç•Œåæ ‡ç‚¹...")
        depth_map = predictions["depth"]  # (S, H, W, 1)
        world_points = unproject_depth_map_to_point_map(depth_map, predictions["extrinsic"], predictions["intrinsic"])
        predictions["world_points_from_depth"] = world_points
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        return predictions
        
    except Exception as e:
        logger.error(f"VGGTæ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        raise e

def _generate_3d_model_from_predictions(predictions: Dict, filename_prefix: str = "3d/vggt_model", 
                                      conf_thres: float = 50.0, 
                                      show_cam: bool = True,
                                      mask_black_bg: bool = False,
                                      mask_white_bg: bool = False,
                                      mask_sky: bool = False) -> tuple:
    """
    æ ¹æ®VGGTé¢„æµ‹ç»“æœç”Ÿæˆ3Dæ¨¡å‹æ–‡ä»¶ï¼ˆGLBæ ¼å¼ï¼‰
    ä½¿ç”¨åŸç‰ˆpredictions_to_glbå‡½æ•°ç¡®ä¿åŸæ±åŸå‘³çš„è¾“å‡ºè´¨é‡ï¼Œä¸é™ä½ç²¾åº¦
    
    Returns:
        tuple: (glb_path, ui_result) - æ–‡ä»¶è·¯å¾„å’ŒUIç»“æœå­—å…¸
    """
    try:
        # æ£€æŸ¥ä¾èµ–
        if not TRIMESH_AVAILABLE:
            logger.error(f"trimeshä¸å¯ç”¨: {_TRIMESH_IMPORT_ERROR}")
            return _generate_3d_model_fallback(predictions, filename_prefix, conf_thres, show_cam, mask_black_bg, mask_white_bg, mask_sky)
        
        if not MATPLOTLIB_AVAILABLE:
            logger.error(f"matplotlibä¸å¯ç”¨: {_MATPLOTLIB_IMPORT_ERROR}")
            return _generate_3d_model_fallback(predictions, filename_prefix, conf_thres, show_cam, mask_black_bg, mask_white_bg, mask_sky)
        
        if not SCIPY_AVAILABLE:
            logger.error(f"scipyä¸å¯ç”¨: {_SCIPY_IMPORT_ERROR}")
            return _generate_3d_model_fallback(predictions, filename_prefix, conf_thres, show_cam, mask_black_bg, mask_white_bg, mask_sky)
        
        # ä½¿ç”¨ComfyUIæ ‡å‡†è·¯å¾„å¤„ç†æ–¹å¼
        if FOLDER_PATHS_AVAILABLE:
            full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(
                filename_prefix, folder_paths.get_output_directory()
            )
        else:
            # å¤‡ç”¨æ–¹æ¡ˆ
            full_output_folder = os.path.join("output", "3d")
            os.makedirs(full_output_folder, exist_ok=True)
            filename = "vggt_model"
            counter = 0
            subfolder = "3d"
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«å‚æ•°ä¿¡æ¯ä½†æ›´ç®€æ´ï¼‰
        param_suffix = f"_conf{conf_thres:.0f}"
        if show_cam:
            param_suffix += "_cam"
        if mask_black_bg:
            param_suffix += "_mbg"
        if mask_white_bg:
            param_suffix += "_mwg"
        if mask_sky:
            param_suffix += "_msky"
            
        glb_filename = f"{filename}_{counter:05}{param_suffix}.glb"
        glb_path = os.path.join(full_output_folder, glb_filename)
        
        logger.info(f"ä½¿ç”¨åŸç‰ˆpredictions_to_glbç”Ÿæˆ3Dæ¨¡å‹ï¼ˆåŸæ±åŸå‘³ï¼Œä¸é™ä½ç²¾åº¦ï¼‰: {glb_path}")
        
        # è°ƒç”¨åŸç‰ˆpredictions_to_glbå‡½æ•° - ä½¿ç”¨åŸæ±åŸå‘³çš„å‚æ•°
        scene_3d = predictions_to_glb(
            predictions,
            conf_thres=conf_thres,
            filter_by_frames="all",  # å¤„ç†æ‰€æœ‰å¸§
            mask_black_bg=mask_black_bg,
            mask_white_bg=mask_white_bg,
            show_cam=show_cam,
            mask_sky=mask_sky,
            target_dir=None,  # ä¸éœ€è¦ä¸­é—´æ–‡ä»¶ç›®å½•ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ComfyUIç¯å¢ƒä¸­
            prediction_mode="Predicted Pointmap"  # ä¼˜å…ˆä½¿ç”¨Pointmapåˆ†æ”¯
        )
        
        # ä¿å­˜trimesh.Sceneä¸ºGLBæ–‡ä»¶ - åŸæ»‹åŸå‘³ï¼Œä¸é™ä½ç²¾åº¦
        logger.info(f"ä¿å­˜åŸç‰ˆtrimesh.Sceneåˆ°GLBæ–‡ä»¶: {glb_path}")
        scene_3d.export(glb_path)
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ›å»º
        if os.path.exists(glb_path):
            file_size = os.path.getsize(glb_path)
            logger.info(f"æˆåŠŸç”ŸæˆåŸç‰ˆè´¨é‡3Dæ¨¡å‹: {glb_path}, æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            # è¿”å›ComfyUIæ ‡å‡†æ ¼å¼çš„ç»“æœ
            ui_result = {
                "filename": glb_filename,
                "subfolder": subfolder,
                "type": "output"
            }
            
            return glb_path, ui_result
        else:
            logger.error(f"GLBæ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {glb_path}")
            return "", {}
            
    except Exception as e:
        logger.error(f"ä½¿ç”¨åŸç‰ˆpredictions_to_glbç”Ÿæˆ3Dæ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # å¦‚æœåŸç‰ˆæ–¹æ³•å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
        logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨GLBç”Ÿæˆæ–¹æ³•...")
        return _generate_3d_model_fallback(predictions, filename_prefix, conf_thres, show_cam, mask_black_bg, mask_white_bg, mask_sky)

def _generate_3d_model_fallback(predictions: Dict, filename_prefix: str = "3d/vggt_model", 
                              conf_thres: float = 50.0, 
                              show_cam: bool = True,
                              mask_black_bg: bool = False,
                              mask_white_bg: bool = False,
                              mask_sky: bool = False) -> tuple:
    """
    å¤‡ç”¨3Dæ¨¡å‹ç”Ÿæˆæ–¹æ³•ï¼ˆå½“åŸç‰ˆæ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
    ä¿æŒåŸæ±åŸå‘³çš„æ•°æ®ï¼Œä¸é™ä½ç²¾åº¦
    """
    try:
        # ä½¿ç”¨ComfyUIæ ‡å‡†è·¯å¾„å¤„ç†æ–¹å¼
        if FOLDER_PATHS_AVAILABLE:
            full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(
                filename_prefix, folder_paths.get_output_directory()
            )
        else:
            # å¤‡ç”¨æ–¹æ¡ˆ
            full_output_folder = os.path.join("output", "3d")
            os.makedirs(full_output_folder, exist_ok=True)
            filename = "vggt_model"
            counter = 0
            subfolder = "3d"
        
        # ç”Ÿæˆæ–‡ä»¶å
        param_suffix = f"_fallback_conf{conf_thres}"
        glb_filename = f"{filename}_{counter:05}_{param_suffix}.glb"
        glb_path = os.path.join(full_output_folder, glb_filename)
        
        # è·å–æ•°æ®
        if "world_points_from_depth" in predictions:
            world_points = predictions["world_points_from_depth"]  # (S, H, W, 3)
        else:
            logger.warning("world_points_from_depth not found, skipping 3D model generation")
            return "", {}
        
        # è·å–å›¾åƒé¢œè‰²ä¿¡æ¯
        images = predictions.get("images", None)
        
        # ç®€å•çš„ç‚¹äº‘å¤„ç† - ä¸é™ä½ç²¾åº¦ï¼Œä¿æŒåŸæ±åŸå‘³
        S, H, W = world_points.shape[:3]
        
        # æŒ‰ç…§åŸç‰ˆé€»è¾‘å¤„ç†
        vertices_3d = world_points.reshape(-1, 3)
        
        # å¤„ç†é¢œè‰²
        if images is not None:
            # å¤„ç†å›¾åƒæ ¼å¼
            if images.ndim == 4 and images.shape[1] == 3:  # NCHW format
                colors_rgb = np.transpose(images, (0, 2, 3, 1))
            else:  # Assume already in NHWC format
                colors_rgb = images
            colors_rgb = (colors_rgb.reshape(-1, 3) * 255).astype(np.uint8)
        else:
            # æ²¡æœ‰é¢œè‰²ä¿¡æ¯ï¼Œç”Ÿæˆé»˜è®¤é¢œè‰²
            colors_rgb = np.ones((len(vertices_3d), 3), dtype=np.uint8) * 128
        
        # ç®€å•çš„ç½®ä¿¡åº¦è¿‡æ»¤ï¼ˆæŒ‰ç…§åŸç‰ˆé€»è¾‘ï¼‰
        if conf_thres > 0:
            # ä½¿ç”¨è·ç¦»ä½œä¸ºç½®ä¿¡åº¦çš„ç®€å•æ›¿ä»£
            center = np.mean(vertices_3d, axis=0)
            distances = np.linalg.norm(vertices_3d - center, axis=1)
            threshold = np.percentile(distances, conf_thres)
            conf_mask = distances <= threshold
        else:
            conf_mask = np.ones(len(vertices_3d), dtype=bool)
        
        # åº”ç”¨èƒŒæ™¯è¿‡æ»¤
        if mask_black_bg:
            black_bg_mask = colors_rgb.sum(axis=1) >= 16
            conf_mask = conf_mask & black_bg_mask
        
        if mask_white_bg:
            white_bg_mask = ~((colors_rgb[:, 0] > 240) & (colors_rgb[:, 1] > 240) & (colors_rgb[:, 2] > 240))
            conf_mask = conf_mask & white_bg_mask
        
        # åº”ç”¨è¿‡æ»¤
        vertices_3d = vertices_3d[conf_mask]
        colors_rgb = colors_rgb[conf_mask]
        
        if len(vertices_3d) == 0:
            vertices_3d = np.array([[1, 0, 0]])
            colors_rgb = np.array([[255, 255, 255]])
        
        # ç”Ÿæˆç®€å•çš„é¢ï¼ˆæ¯3ä¸ªç‚¹ç»„æˆä¸€ä¸ªä¸‰è§’å½¢ï¼‰
        n_points = len(vertices_3d)
        n_triangles = max(1, n_points // 3)
        faces = []
        for i in range(n_triangles):
            if i*3+2 < n_points:
                faces.append([i*3, i*3+1, i*3+2])
        
        if not faces:
            faces = [[0, 0, 0]]  # è‡³å°‘ä¸€ä¸ªé¢
        
        faces = np.array(faces, dtype=np.uint32)
        
        logger.info(f"å¤‡ç”¨æ–¹æ³•ç”Ÿæˆäº† {len(vertices_3d)} ä¸ªé¡¶ç‚¹å’Œ {len(faces)} ä¸ªé¢ï¼ˆä¿æŒåŸå§‹ç²¾åº¦ï¼‰")
        
        # ç”Ÿæˆå…ƒæ•°æ®
        metadata = {
            "source": "VGGT-Fallback",
            "confidence_threshold": conf_thres,
            "show_cameras": show_cam,
            "num_vertices": len(vertices_3d),
            "num_faces": len(faces),
            "method": "fallback_original_quality"
        }
        
        # ä½¿ç”¨ç®€åŒ–çš„GLBä¿å­˜æ–¹æ³•
        save_glb_simple(vertices_3d, faces, glb_path, metadata)
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ›å»º
        if os.path.exists(glb_path):
            file_size = os.path.getsize(glb_path)
            logger.info(f"å¤‡ç”¨æ–¹æ³•æˆåŠŸç”Ÿæˆ3Dæ¨¡å‹: {glb_path}, æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            # è¿”å›ComfyUIæ ‡å‡†æ ¼å¼çš„ç»“æœ
            ui_result = {
                "filename": glb_filename,
                "subfolder": subfolder,
                "type": "output"
            }
            
            return glb_path, ui_result
        else:
            logger.error(f"å¤‡ç”¨æ–¹æ³•GLBæ–‡ä»¶ä¿å­˜å¤±è´¥")
            return "", {}
            
    except Exception as e:
        logger.error(f"å¤‡ç”¨3Dæ¨¡å‹ç”Ÿæˆæ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
        return "", {}

# -----------------------------------------------------------------------------
# ä¸»è¦èŠ‚ç‚¹å®ç°
# -----------------------------------------------------------------------------

class VGGTMultiInputNode:
    """VGGT å¤šè¾“å…¥ç›¸æœºå‚æ•°ä¼°è®¡èŠ‚ç‚¹ - æ”¯æŒè§†é¢‘å’Œå›¾ç‰‡åºåˆ—è¾“å…¥"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "vggt_model": ("VVL_VGGT_MODEL", {
                    "tooltip": "æ¥è‡ªVVLVGGTLoaderçš„VGGTæ¨¡å‹å®ä¾‹ï¼ŒåŒ…å«å·²åŠ è½½çš„æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯"
                }),
            },
            "optional": {
                "video": (IO.VIDEO, {
                    "tooltip": "è§†é¢‘è¾“å…¥ï¼ˆå¯é€‰ï¼‰"
                }),
                "images": ("IMAGE", {
                    "tooltip": "å›¾ç‰‡åºåˆ—è¾“å…¥ï¼ˆå¯é€‰ï¼‰"
                }),
                "confidence_threshold": ("FLOAT", {
                    "default": 50.0, "min": 0.0, "max": 100.0, "step": 0.1,
                    "tooltip": "ç½®ä¿¡åº¦é˜ˆå€¼(%)ï¼Œç”¨äºè¿‡æ»¤3Dç‚¹äº‘ä¸­çš„ä½ç½®ä¿¡åº¦ç‚¹"
                }),
                "show_cameras": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦åœ¨3Dæ¨¡å‹ä¸­æ˜¾ç¤ºç›¸æœºä½ç½®"
                }),
                "mask_black_bg": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦è¿‡æ»¤é»‘è‰²èƒŒæ™¯ç‚¹"
                }),
                "mask_white_bg": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦è¿‡æ»¤ç™½è‰²èƒŒæ™¯ç‚¹"
                }),
                "mask_sky": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦è¿‡æ»¤å¤©ç©ºç‚¹"
                }),
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING", "STRING")
    RETURN_NAMES = ("intrinsics_json", "trajectory_preview", "poses_json", "model_3d_path")
    OUTPUT_TOOLTIPS = [
        "ç›¸æœºå†…å‚æ•°æ® (JSONæ ¼å¼)",
        "ç›¸æœºè½¨è¿¹2Dé¢„è§ˆå›¾åƒ",
        "ç›¸æœºä½å§¿æ•°æ® (JSONæ ¼å¼)",
        "3Dæ¨¡å‹æ–‡ä»¶è·¯å¾„ (å¯è¿æ¥åˆ°Preview3Dæˆ–å…¶ä»–èŠ‚ç‚¹)"
    ]
    OUTPUT_NODE = True
    FUNCTION = "estimate_multi_input"
    CATEGORY = "ğŸ’ƒVVL/VGGT"

    def _resolve_video_path(self, video: Any) -> str:
        """è§£æè§†é¢‘è·¯å¾„"""
        if video is None:
            return ""
        # å¦‚æœ video æ˜¯å­—ç¬¦ä¸²
        if isinstance(video, str):
            return video
        # å¸¸è§å±æ€§
        attrs = ["_VideoFromFile__file", "path", "video_path", "_path", "file_path"]
        for attr in attrs:
            if hasattr(video, attr):
                val = getattr(video, attr)
                if isinstance(val, str):
                    return val
        return ""

    def _extract_video_frames_original(self, video_path: str) -> List[np.ndarray]:
        """æŒ‰ç…§åŸç‰ˆGradioæ–¹å¼æå–è§†é¢‘å¸§ï¼šå›ºå®šæ¯ç§’1å¸§ï¼Œæ— å¸§æ•°é™åˆ¶"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_interval = int(fps * 1)  # æ¯ç§’1å¸§ï¼Œä¸åŸç‰ˆGradioä¸€è‡´
        
        frames = []
        count = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            count += 1
            if count % frame_interval == 0:
                frames.append(frame.copy())
        
        cap.release()
        logger.info(f"VGGTMultiInputNode: æŒ‰åŸç‰ˆæ–¹å¼æå–äº† {len(frames)} å¸§ (æ¯ç§’1å¸§)")
        return frames

    def estimate_multi_input(self, vggt_model: Dict, 
                           video=None, images=None,
                           confidence_threshold: float = 50.0,
                           show_cameras: bool = True,
                           mask_black_bg: bool = False,
                           mask_white_bg: bool = False,
                           mask_sky: bool = False):
        """å¤šè¾“å…¥æ–¹å¼çš„ç›¸æœºå‚æ•°ä¼°è®¡"""
        try:
            # æ£€æŸ¥VGGTå·¥å…·å‡½æ•°æ˜¯å¦å¯ç”¨
            if not VGGT_UTILS_AVAILABLE:
                raise RuntimeError(f"VGGT utils not available: {_VGGT_UTILS_IMPORT_ERROR}")
            
            # ä»æ¨¡å‹å­—å…¸ä¸­è·å–ä¿¡æ¯
            model_instance = vggt_model['model']
            device = vggt_model['device']
            model_name = vggt_model['model_name']
            
            logger.info(f"VGGTMultiInputNode: Using {model_name} on {device}")
            
            # ç¡®å®šè¾“å…¥æºå’Œå¤„ç†æ–¹å¼
            img_paths = []
            
            # å¤„ç†å›¾ç‰‡åºåˆ—è¾“å…¥
            if images is not None and images.shape[0] > 0:
                logger.info(f"VGGTMultiInputNode: å¤„ç†å›¾ç‰‡åºåˆ—è¾“å…¥ï¼Œæ•°é‡: {images.shape[0]}")
                
                # ä¿å­˜å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶
                with tempfile.TemporaryDirectory() as tmpdir:
                    for i in range(images.shape[0]):
                        img_tensor = images[i]
                        
                        # ç¡®ä¿æ•°å€¼èŒƒå›´æ­£ç¡®
                        if img_tensor.max() <= 1.0:
                            img_np = (img_tensor.cpu().numpy() * 255).astype(np.uint8)
                        else:
                            img_np = img_tensor.cpu().numpy().astype(np.uint8)
                        
                        img_path = os.path.join(tmpdir, f"image_{i:04d}.png")
                        Image.fromarray(img_np).save(img_path)
                        img_paths.append(img_path)
                    
                    # è¿è¡Œæ¨ç†
                    predictions = _run_vggt_model_inference(img_paths, model_instance, device)
            
            # å¤„ç†è§†é¢‘è¾“å…¥
            elif video is not None:
                vid_path = self._resolve_video_path(video)
                if not vid_path or not os.path.exists(vid_path):
                    raise FileNotFoundError(f"æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶: {vid_path}")
                
                logger.info(f"VGGTMultiInputNode: å¤„ç†è§†é¢‘è¾“å…¥: {vid_path}")
                
                # ä½¿ç”¨åŸç‰ˆGradioæ–¹å¼æå–è§†é¢‘å¸§
                frames = self._extract_video_frames_original(vid_path)
                if not frames:
                    raise RuntimeError("æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§")
                
                # ä¿å­˜å¸§åˆ°ä¸´æ—¶æ–‡ä»¶
                with tempfile.TemporaryDirectory() as tmpdir:
                    for i, frame in enumerate(frames):
                        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        img_path = os.path.join(tmpdir, f"frame_{i:04d}.png")
                        Image.fromarray(rgb).save(img_path)
                        img_paths.append(img_path)
                    
                    # è¿è¡Œæ¨ç†
                    predictions = _run_vggt_model_inference(img_paths, model_instance, device)
            
            else:
                raise ValueError("å¿…é¡»æä¾›è§†é¢‘æˆ–å›¾ç‰‡åºåˆ—è¾“å…¥")
            
            # ç”Ÿæˆ3Dæ¨¡å‹æ–‡ä»¶
            model_3d_path, ui_result = _generate_3d_model_from_predictions(
                predictions, filename_prefix="3d/vggt_model",
                conf_thres=confidence_threshold,
                show_cam=show_cameras,
                mask_black_bg=mask_black_bg,
                mask_white_bg=mask_white_bg,
                mask_sky=mask_sky
            )

            # ç”ŸæˆJSONè¾“å‡º
            source_type = "images" if images is not None else "video"
            intrinsics_json, poses_json = _matrices_to_json(predictions["intrinsic"], predictions["extrinsic"], source_type)

            # ç”Ÿæˆè½¨è¿¹é¢„è§ˆå›¾
            extrinsic_tensor = torch.from_numpy(predictions["extrinsic"]).float()
            traj_tensor = _create_traj_preview(extrinsic_tensor)

            logger.info("VGGTMultiInputNode: Camera estimation completed successfully")
            
            # å‡†å¤‡è¿”å›çš„3Dæ¨¡å‹è·¯å¾„
            if model_3d_path and os.path.exists(model_3d_path):
                # ä½¿ç”¨ComfyUIçš„å¸¦æ³¨é‡Šè·¯å¾„æ ¼å¼
                if FOLDER_PATHS_AVAILABLE:
                    try:
                        # ä½¿ç”¨folder_paths.get_annotated_filepathæ¥ç”Ÿæˆæ­£ç¡®çš„è·¯å¾„æ ¼å¼
                        annotated_path = folder_paths.get_annotated_filepath(model_3d_path)
                        model_output_path = annotated_path
                    except:
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨ç”Ÿæˆç›¸å¯¹è·¯å¾„
                        relative_path = os.path.relpath(model_3d_path, folder_paths.get_output_directory())
                        model_output_path = f"{relative_path} [output]"
                else:
                    model_output_path = model_3d_path
            else:
                model_output_path = ""
            
            # è¿”å›ç»“æœï¼ŒåŒ…æ‹¬UIç»“æœç”¨äº3Dæ¨¡å‹é¢„è§ˆå’Œç›´æ¥çš„æ–‡ä»¶è·¯å¾„
            result = (intrinsics_json, traj_tensor, poses_json, model_output_path)
            if ui_result:
                return {"ui": {"3d": [ui_result]}, "result": result}
            else:
                return {"result": result}

        except Exception as e:
            error_msg = f"VGGTå¤šè¾“å…¥ä¼°è®¡é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            
            # è¿”å›é”™è¯¯ç»“æœ
            empty_img = torch.ones((1, 400, 400, 3), dtype=torch.float32) * 0.1
            error_json = json.dumps({"success": False, "error": error_msg}, ensure_ascii=False, indent=2)
            return {"result": (error_json, empty_img, error_json, "")}

# -----------------------------------------------------------------------------
# åŸç‰ˆVGGTçš„predictions_to_glbå‡½æ•°å’Œæ‰€æœ‰è¾…åŠ©å‡½æ•°ï¼ˆå®Œæ•´ç§»æ¤è‡ªvisual_util.pyï¼‰
# -----------------------------------------------------------------------------

def predictions_to_glb(
    predictions,
    conf_thres=50.0,
    filter_by_frames="all",
    mask_black_bg=False,
    mask_white_bg=False,
    show_cam=True,
    mask_sky=False,
    target_dir=None,
    prediction_mode="Predicted Pointmap",
) -> "trimesh.Scene":
    """
    Converts VGGT predictions to a 3D scene represented as a GLB file.

    Args:
        predictions (dict): Dictionary containing model predictions with keys:
            - world_points: 3D point coordinates (S, H, W, 3)
            - world_points_conf: Confidence scores (S, H, W)
            - images: Input images (S, H, W, 3)
            - extrinsic: Camera extrinsic matrices (S, 3, 4)
        conf_thres (float): Percentage of low-confidence points to filter out (default: 50.0)
        filter_by_frames (str): Frame filter specification (default: "all")
        mask_black_bg (bool): Mask out black background pixels (default: False)
        mask_white_bg (bool): Mask out white background pixels (default: False)
        show_cam (bool): Include camera visualization (default: True)
        mask_sky (bool): Apply sky segmentation mask (default: False)
        target_dir (str): Output directory for intermediate files (default: None)
        prediction_mode (str): Prediction mode selector (default: "Predicted Pointmap")

    Returns:
        trimesh.Scene: Processed 3D scene containing point cloud and cameras

    Raises:
        ValueError: If input predictions structure is invalid
    """
    if not isinstance(predictions, dict):
        raise ValueError("predictions must be a dictionary")

    if conf_thres is None:
        conf_thres = 10.0

    print("Building GLB scene")
    selected_frame_idx = None
    if filter_by_frames != "all" and filter_by_frames != "All":
        try:
            # Extract the index part before the colon
            selected_frame_idx = int(filter_by_frames.split(":")[0])
        except (ValueError, IndexError):
            pass

    if "Pointmap" in prediction_mode:
        print("Using Pointmap Branch")
        if "world_points" in predictions:
            pred_world_points = predictions["world_points"]  # No batch dimension to remove
            pred_world_points_conf = predictions.get("world_points_conf", np.ones_like(pred_world_points[..., 0]))
        else:
            print("Warning: world_points not found in predictions, falling back to depth-based points")
            pred_world_points = predictions["world_points_from_depth"]
            pred_world_points_conf = predictions.get("depth_conf", np.ones_like(pred_world_points[..., 0]))
    else:
        print("Using Depthmap and Camera Branch")
        pred_world_points = predictions["world_points_from_depth"]
        pred_world_points_conf = predictions.get("depth_conf", np.ones_like(pred_world_points[..., 0]))

    # Get images from predictions
    images = predictions["images"]
    # Use extrinsic matrices instead of pred_extrinsic_list
    camera_matrices = predictions["extrinsic"]

    if mask_sky:
        if target_dir is not None:
            import onnxruntime

            skyseg_session = None
            target_dir_images = target_dir + "/images"
            image_list = sorted(os.listdir(target_dir_images))
            sky_mask_list = []

            # Get the shape of pred_world_points_conf to match
            S, H, W = (
                pred_world_points_conf.shape
                if hasattr(pred_world_points_conf, "shape")
                else (len(images), images.shape[1], images.shape[2])
            )

            # Download skyseg.onnx if it doesn't exist
            if not os.path.exists("skyseg.onnx"):
                print("Downloading skyseg.onnx...")
                download_file_from_url(
                    "https://huggingface.co/JianyuanWang/skyseg/resolve/main/skyseg.onnx", "skyseg.onnx"
                )

            for i, image_name in enumerate(image_list):
                image_filepath = os.path.join(target_dir_images, image_name)
                mask_filepath = os.path.join(target_dir, "sky_masks", image_name)

                # Check if mask already exists
                if os.path.exists(mask_filepath):
                    # Load existing mask
                    sky_mask = cv2.imread(mask_filepath, cv2.IMREAD_GRAYSCALE)
                else:
                    # Generate new mask
                    if skyseg_session is None:
                        skyseg_session = onnxruntime.InferenceSession("skyseg.onnx")
                    sky_mask = segment_sky(image_filepath, skyseg_session, mask_filepath)

                # Resize mask to match HÃ—W if needed
                if sky_mask.shape[0] != H or sky_mask.shape[1] != W:
                    sky_mask = cv2.resize(sky_mask, (W, H))

                sky_mask_list.append(sky_mask)

            # Convert list to numpy array with shape SÃ—HÃ—W
            sky_mask_array = np.array(sky_mask_list)

            # Apply sky mask to confidence scores
            sky_mask_binary = (sky_mask_array > 0.1).astype(np.float32)
            pred_world_points_conf = pred_world_points_conf * sky_mask_binary

    if selected_frame_idx is not None:
        pred_world_points = pred_world_points[selected_frame_idx][None]
        pred_world_points_conf = pred_world_points_conf[selected_frame_idx][None]
        images = images[selected_frame_idx][None]
        camera_matrices = camera_matrices[selected_frame_idx][None]

    vertices_3d = pred_world_points.reshape(-1, 3)
    # Handle different image formats - check if images need transposing
    if images.ndim == 4 and images.shape[1] == 3:  # NCHW format
        colors_rgb = np.transpose(images, (0, 2, 3, 1))
    else:  # Assume already in NHWC format
        colors_rgb = images
    colors_rgb = (colors_rgb.reshape(-1, 3) * 255).astype(np.uint8)

    conf = pred_world_points_conf.reshape(-1)
    # Convert percentage threshold to actual confidence value
    if conf_thres == 0.0:
        conf_threshold = 0.0
    else:
        conf_threshold = np.percentile(conf, conf_thres)

    conf_mask = (conf >= conf_threshold) & (conf > 1e-5)

    if mask_black_bg:
        black_bg_mask = colors_rgb.sum(axis=1) >= 16
        conf_mask = conf_mask & black_bg_mask

    if mask_white_bg:
        # Filter out white background pixels (RGB values close to white)
        # Consider pixels white if all RGB values are above 240
        white_bg_mask = ~((colors_rgb[:, 0] > 240) & (colors_rgb[:, 1] > 240) & (colors_rgb[:, 2] > 240))
        conf_mask = conf_mask & white_bg_mask

    vertices_3d = vertices_3d[conf_mask]
    colors_rgb = colors_rgb[conf_mask]

    if vertices_3d is None or np.asarray(vertices_3d).size == 0:
        vertices_3d = np.array([[1, 0, 0]])
        colors_rgb = np.array([[255, 255, 255]])
        scene_scale = 1
    else:
        # Calculate the 5th and 95th percentiles along each axis
        lower_percentile = np.percentile(vertices_3d, 5, axis=0)
        upper_percentile = np.percentile(vertices_3d, 95, axis=0)

        # Calculate the diagonal length of the percentile bounding box
        scene_scale = np.linalg.norm(upper_percentile - lower_percentile)

    colormap = matplotlib.colormaps.get_cmap("gist_rainbow")

    # Initialize a 3D scene
    scene_3d = trimesh.Scene()

    # Add point cloud data to the scene
    point_cloud_data = trimesh.PointCloud(vertices=vertices_3d, colors=colors_rgb)

    scene_3d.add_geometry(point_cloud_data)

    # Prepare 4x4 matrices for camera extrinsics
    num_cameras = len(camera_matrices)
    extrinsics_matrices = np.zeros((num_cameras, 4, 4))
    extrinsics_matrices[:, :3, :4] = camera_matrices
    extrinsics_matrices[:, 3, 3] = 1

    if show_cam:
        # Add camera models to the scene
        for i in range(num_cameras):
            world_to_camera = extrinsics_matrices[i]
            camera_to_world = np.linalg.inv(world_to_camera)
            rgba_color = colormap(i / num_cameras)
            current_color = tuple(int(255 * x) for x in rgba_color[:3])

            integrate_camera_into_scene(scene_3d, camera_to_world, current_color, scene_scale)

    # Align scene to the observation of the first camera
    scene_3d = apply_scene_alignment(scene_3d, extrinsics_matrices)

    print("GLB Scene built")
    return scene_3d


def integrate_camera_into_scene(scene: "trimesh.Scene", transform: np.ndarray, face_colors: tuple, scene_scale: float):
    """
    Integrates a fake camera mesh into the 3D scene.

    Args:
        scene (trimesh.Scene): The 3D scene to add the camera model.
        transform (np.ndarray): Transformation matrix for camera positioning.
        face_colors (tuple): Color of the camera face.
        scene_scale (float): Scale of the scene.
    """

    cam_width = scene_scale * 0.05
    cam_height = scene_scale * 0.1

    # Create cone shape for camera
    rot_45_degree = np.eye(4)
    rot_45_degree[:3, :3] = Rotation.from_euler("z", 45, degrees=True).as_matrix()
    rot_45_degree[2, 3] = -cam_height

    opengl_transform = get_opengl_conversion_matrix()
    # Combine transformations
    complete_transform = transform @ opengl_transform @ rot_45_degree
    camera_cone_shape = trimesh.creation.cone(cam_width, cam_height, sections=4)

    # Generate mesh for the camera
    slight_rotation = np.eye(4)
    slight_rotation[:3, :3] = Rotation.from_euler("z", 2, degrees=True).as_matrix()

    vertices_combined = np.concatenate(
        [
            camera_cone_shape.vertices,
            0.95 * camera_cone_shape.vertices,
            transform_points(slight_rotation, camera_cone_shape.vertices),
        ]
    )
    vertices_transformed = transform_points(complete_transform, vertices_combined)

    mesh_faces = compute_camera_faces(camera_cone_shape)

    # Add the camera mesh to the scene
    camera_mesh = trimesh.Trimesh(vertices=vertices_transformed, faces=mesh_faces)
    camera_mesh.visual.face_colors[:, :3] = face_colors
    scene.add_geometry(camera_mesh)


def apply_scene_alignment(scene_3d: "trimesh.Scene", extrinsics_matrices: np.ndarray) -> "trimesh.Scene":
    """
    Aligns the 3D scene based on the extrinsics of the first camera.

    Args:
        scene_3d (trimesh.Scene): The 3D scene to be aligned.
        extrinsics_matrices (np.ndarray): Camera extrinsic matrices.

    Returns:
        trimesh.Scene: Aligned 3D scene.
    """
    # Set transformations for scene alignment
    opengl_conversion_matrix = get_opengl_conversion_matrix()

    # Rotation matrix for alignment (180 degrees around the y-axis)
    align_rotation = np.eye(4)
    align_rotation[:3, :3] = Rotation.from_euler("y", 180, degrees=True).as_matrix()

    # Apply transformation
    initial_transformation = np.linalg.inv(extrinsics_matrices[0]) @ opengl_conversion_matrix @ align_rotation
    scene_3d.apply_transform(initial_transformation)
    return scene_3d


def get_opengl_conversion_matrix() -> np.ndarray:
    """
    Constructs and returns the OpenGL conversion matrix.

    Returns:
        numpy.ndarray: A 4x4 OpenGL conversion matrix.
    """
    # Create an identity matrix
    matrix = np.identity(4)

    # Flip the y and z axes
    matrix[1, 1] = -1
    matrix[2, 2] = -1

    return matrix


def transform_points(transformation: np.ndarray, points: np.ndarray, dim: int = None) -> np.ndarray:
    """
    Applies a 4x4 transformation to a set of points.

    Args:
        transformation (np.ndarray): Transformation matrix.
        points (np.ndarray): Points to be transformed.
        dim (int, optional): Dimension for reshaping the result.

    Returns:
        np.ndarray: Transformed points.
    """
    points = np.asarray(points)
    initial_shape = points.shape[:-1]
    dim = dim or points.shape[-1]

    # Apply transformation
    transformation = transformation.swapaxes(-1, -2)  # Transpose the transformation matrix
    points = points @ transformation[..., :-1, :] + transformation[..., -1:, :]

    # Reshape the result
    result = points[..., :dim].reshape(*initial_shape, dim)
    return result


def compute_camera_faces(cone_shape: "trimesh.Trimesh") -> np.ndarray:
    """
    Computes the faces for the camera mesh.

    Args:
        cone_shape (trimesh.Trimesh): The shape of the camera cone.

    Returns:
        np.ndarray: Array of faces for the camera mesh.
    """
    # Create pseudo cameras
    faces_list = []
    num_vertices_cone = len(cone_shape.vertices)

    for face in cone_shape.faces:
        if 0 in face:
            continue
        v1, v2, v3 = face
        v1_offset, v2_offset, v3_offset = face + num_vertices_cone
        v1_offset_2, v2_offset_2, v3_offset_2 = face + 2 * num_vertices_cone

        faces_list.extend(
            [
                (v1, v2, v2_offset),
                (v1, v1_offset, v3),
                (v3_offset, v2, v3),
                (v1, v2, v2_offset_2),
                (v1, v1_offset_2, v3),
                (v3_offset_2, v2, v3),
            ]
        )

    faces_list += [(v3, v2, v1) for v1, v2, v3 in faces_list]
    return np.array(faces_list)


def segment_sky(image_path, onnx_session, mask_filename=None):
    """
    Segments sky from an image using an ONNX model.
    Thanks for the great model provided by https://github.com/xiongzhu666/Sky-Segmentation-and-Post-processing

    Args:
        image_path: Path to input image
        onnx_session: ONNX runtime session with loaded model
        mask_filename: Path to save the output mask

    Returns:
        np.ndarray: Binary mask where 255 indicates non-sky regions
    """

    assert mask_filename is not None
    image = cv2.imread(image_path)

    result_map = run_skyseg(onnx_session, [320, 320], image)
    # resize the result_map to the original image size
    result_map_original = cv2.resize(result_map, (image.shape[1], image.shape[0]))

    # Fix: Invert the mask so that 255 = non-sky, 0 = sky
    # The model outputs low values for sky, high values for non-sky
    output_mask = np.zeros_like(result_map_original)
    output_mask[result_map_original < 32] = 255  # Use threshold of 32

    os.makedirs(os.path.dirname(mask_filename), exist_ok=True)
    cv2.imwrite(mask_filename, output_mask)
    return output_mask


def run_skyseg(onnx_session, input_size, image):
    """
    Runs sky segmentation inference using ONNX model.

    Args:
        onnx_session: ONNX runtime session
        input_size: Target size for model input (width, height)
        image: Input image in BGR format

    Returns:
        np.ndarray: Segmentation mask
    """

    # Pre process:Resize, BGR->RGB, Transpose, PyTorch standardization, float32 cast
    temp_image = copy.deepcopy(image)
    resize_image = cv2.resize(temp_image, dsize=(input_size[0], input_size[1]))
    x = cv2.cvtColor(resize_image, cv2.COLOR_BGR2RGB)
    x = np.array(x, dtype=np.float32)
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    x = (x / 255 - mean) / std
    x = x.transpose(2, 0, 1)
    x = x.reshape(-1, 3, input_size[0], input_size[1]).astype("float32")

    # Inference
    input_name = onnx_session.get_inputs()[0].name
    output_name = onnx_session.get_outputs()[0].name
    onnx_result = onnx_session.run([output_name], {input_name: x})

    # Post process
    onnx_result = np.array(onnx_result).squeeze()
    min_value = np.min(onnx_result)
    max_value = np.max(onnx_result)
    onnx_result = (onnx_result - min_value) / (max_value - min_value)
    onnx_result *= 255
    onnx_result = onnx_result.astype("uint8")

    return onnx_result


def download_file_from_url(url, filename):
    """Downloads a file from a Hugging Face model repo, handling redirects."""
    try:
        # Get the redirect URL
        response = requests.get(url, allow_redirects=False)
        response.raise_for_status()  # Raise HTTPError for bad requests (4xx or 5xx)

        if response.status_code == 302:  # Expecting a redirect
            redirect_url = response.headers["Location"]
            response = requests.get(redirect_url, stream=True)
            response.raise_for_status()
        else:
            print(f"Unexpected status code: {response.status_code}")
            return

        with open(filename, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"Downloaded {filename} successfully.")

    except requests.exceptions.RequestException as e:
        print(f"Error downloading file: {e}")

# -----------------------------------------------------------------------------
# ç®€åŒ–çš„GLBç”Ÿæˆå‡½æ•°ï¼ˆä»…ä½œä¸ºå¤‡ç”¨ï¼‰
# -----------------------------------------------------------------------------

def save_glb_simple(vertices, faces, filepath, metadata=None):
    """
    ç®€åŒ–çš„GLBä¿å­˜å‡½æ•°ï¼ˆå¤‡ç”¨ï¼‰
    """
    try:
        logger.info(f"save_glb_simple: å¼€å§‹ä¿å­˜GLBæ–‡ä»¶åˆ° {filepath}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(filepath)
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if isinstance(vertices, torch.Tensor):
            vertices = vertices.cpu().numpy()
        if isinstance(faces, torch.Tensor):
            faces = faces.cpu().numpy()
        
        vertices_np = vertices.astype(np.float32)
        faces_np = faces.astype(np.uint32)
    
        vertices_buffer = vertices_np.tobytes()
        indices_buffer = faces_np.tobytes()

        def pad_to_4_bytes(buffer):
            padding_length = (4 - (len(buffer) % 4)) % 4
            return buffer + b'\x00' * padding_length

        vertices_buffer_padded = pad_to_4_bytes(vertices_buffer)
        indices_buffer_padded = pad_to_4_bytes(indices_buffer)

        buffer_data = vertices_buffer_padded + indices_buffer_padded

        vertices_byte_length = len(vertices_buffer)
        vertices_byte_offset = 0
        indices_byte_length = len(indices_buffer)
        indices_byte_offset = len(vertices_buffer_padded)

        gltf = {
            "asset": {"version": "2.0", "generator": "ComfyUI-VGGT"},
            "buffers": [
                {
                    "byteLength": len(buffer_data)
                }
            ],
            "bufferViews": [
                {
                    "buffer": 0,
                    "byteOffset": vertices_byte_offset,
                    "byteLength": vertices_byte_length,
                    "target": 34962  # ARRAY_BUFFER
                },
                {
                    "buffer": 0,
                    "byteOffset": indices_byte_offset,
                    "byteLength": indices_byte_length,
                    "target": 34963  # ELEMENT_ARRAY_BUFFER
                }
            ],
            "accessors": [
                {
                    "bufferView": 0,
                    "byteOffset": 0,
                    "componentType": 5126,  # FLOAT
                    "count": len(vertices_np),
                    "type": "VEC3",
                    "max": vertices_np.max(axis=0).tolist(),
                    "min": vertices_np.min(axis=0).tolist()
                },
                {
                    "bufferView": 1,
                    "byteOffset": 0,
                    "componentType": 5125,  # UNSIGNED_INT
                    "count": faces_np.size,
                    "type": "SCALAR"
                }
            ],
            "meshes": [
                {
                    "primitives": [
                        {
                            "attributes": {
                                "POSITION": 0
                            },
                            "indices": 1,
                            "mode": 4  # TRIANGLES
                        }
                    ]
                }
            ],
            "nodes": [
                {
                    "mesh": 0
                }
            ],
            "scenes": [
                {
                    "nodes": [0]
                }
            ],
            "scene": 0
        }

        if metadata is not None:
            gltf["asset"]["extras"] = metadata

        # Convert the JSON to bytes
        gltf_json = json.dumps(gltf).encode('utf8')

        def pad_json_to_4_bytes(buffer):
            padding_length = (4 - (len(buffer) % 4)) % 4
            return buffer + b' ' * padding_length

        gltf_json_padded = pad_json_to_4_bytes(gltf_json)

        # Create the GLB header
        # Magic glTF
        glb_header = struct.pack('<4sII', b'glTF', 2, 12 + 8 + len(gltf_json_padded) + 8 + len(buffer_data))

        # Create JSON chunk header (chunk type 0)
        json_chunk_header = struct.pack('<II', len(gltf_json_padded), 0x4E4F534A)  # "JSON" in little endian

        # Create BIN chunk header (chunk type 1)
        bin_chunk_header = struct.pack('<II', len(buffer_data), 0x004E4942)  # "BIN\0" in little endian

        # Write the GLB file
        with open(filepath, 'wb') as f:
            f.write(glb_header)
            f.write(json_chunk_header)
            f.write(gltf_json_padded)
            f.write(bin_chunk_header)
            f.write(buffer_data)

        logger.info(f"save_glb_simple: GLBæ–‡ä»¶å†™å…¥å®Œæˆ")
        return filepath
    
    except Exception as e:
        logger.error(f"save_glb_simple: ä¿å­˜å¤±è´¥ {e}")
        raise e

# -----------------------------------------------------------------------------
# èŠ‚ç‚¹æ³¨å†Œ
# -----------------------------------------------------------------------------

NODE_CLASS_MAPPINGS = {
    "VGGTMultiInputNode": VGGTMultiInputNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VGGTMultiInputNode": "VVL VGGT Multi Input Camera Estimator",
}

# å¦‚æœæ¨¡å‹åŠ è½½å™¨å¯ç”¨ï¼Œæ·»åŠ åˆ°æ˜ å°„ä¸­
if MODEL_LOADER_AVAILABLE:
    NODE_CLASS_MAPPINGS["VVLVGGTLoader"] = VVLVGGTLoader
    NODE_DISPLAY_NAME_MAPPINGS["VVLVGGTLoader"] = "VVL VGGT Model Loader" 